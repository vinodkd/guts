**Sep-07-2013 18:55 :** Need to reorg dir structure to:

		guts/
			MASTER:
				README.md
				ref/
				src/
					mmap/
						guts.mm
					book/
						01_size.md
						jpg files
				tools/
					readme2index.sh   -- writes out any change to readme as index.html on web/out
					book2html.sh      -- writes out any change to the book onto web/out
					publish.sh        -- copies web/out over to gh-pages
					toHTML.js
					node_modules/				
				web/
					partial/
						book.top.html
						book.bot.html
						index.top.html
						index.bot.html
					css/
						guts.css
					out/   -- this is the source for gh-pages that is generated by the tools.
						index.html
						mmap/
						book/
							01_size.html
			GH-PAGES
				index.html
				mmap/       -- these are the only dirs that will be directly updated in gh-pages. because its pointless to do it in master. 
					        -- Freemind doesnt have a command line option.
					full/
					basic/
				book/

this requires:

* DONE move src to top
* DONE move book under src
* DONE move files in src to book
* DONE create src/mmap and move guts.mm there
* DONE rename toHtml.sh to book2html.sh
* DONEcreate readme2index.sh to convert readme.md into index.html
	- DONE create index.top.html from current gh-pages version
	- DONE create index.bot.html
* DONE create web, web/partial
* DONE mv html/top.html and bot.html to web/partial/book.top.html and bot.html resp
* DONE mv html/guts.css to web/css/guts.css
* DONE delete book/html
* DONE change book2html to use the right directories
* DONE move gh-pages/out to gh-pages/mmap
* DONE create gh-pages/book
* DONE fixed refs to the mmap in readme.md so that when its published, it will refer to the right location.
* DONE create publish.sh that will copy web/out to gh-pages

**Sep-07-2013 19:50 :** Used http://oli.jp/2011/github-pages-workflow/ for help on gh-pages workflow. esp, how to checkout a file from a different branch.

**Sep-25-2013 18:14 :** : I decided to not expand on the specific gravity idea cos i couldnt find theoretical ground for it. the wish is there, the math doesnt support it. Saved the text to a new file therefore.

**Sep-30-2013 08:22 :** TODO: figure out how to resume numbering in markdown. required for questions in hw section.

**Oct-02-2013 08:28 :** As I start writing out the section on data size, i feel two things: one, that data might be a chapter by itself, and two: that i might have to restructure the size chapter into 3: one about program size - an overview, one about code size (the current chapter 1) and a third about data size. alternatively, i could keep each chapter about one metric and restructure the chapter to read as: sect 1: program = algos + ds, sec2: code size, sec2: data size. i think i'll go with the latter approach, but keep data size separate for now so that folks reviewing the code size text will not have too much churn.

**Oct-02-2013 18:16 :** create a poc dir and create a `size.sh`. this should apply sloc, turing-simple and turing-exact sizes to a given codebase after figuring out the filetype.

**Oct-03-2013 08:31 :**  decided to change poc to app - the guts app that sizes things. So new cli:

		guts.sh size --all|turing|sloc dir|file

that is, there's a primary guts app, that takes a measure param - currently only size - which then takes another param to include all size measures or just the turing or just the sloc; and a final param for the file or dir to apply the measure on. sloc will be calculated via a call to an external sloc counting tool.

**Oct-07-2013 08:14 :** Now that I have basic comparison tools for sloc v turings, i tested it out on a java class and found some interesting observations:

* The sizes were sloc=184, turing=163. so numerically pretty close.
* since my logic is basic, it had some interesting side effects:
** anonymous inner classes were considered as statements with containers embedded - as they should. So the simple rule of counting {'s a and ;'s was enough to capture this somewhat corner case and "double count" the ; at the end.
** semicolons in comments were treated as code. This was interesting in that sometimes code is commented out to avoid it from being executed, but retained in source to "use later" or "reference later". this simple method of counting accidentally measured such "latent" code as well :)
** the `package` statement was also counted in the size. This is really metadata, not code; so i definitely need to shore up my thoughts about data

**Oct-18-2013 08:00 :** Been thinking about restructuring the book a little bit based on feedback from friends. The original plan was to have one chapter per measure, size being the first. But as I write it i'm finding that its too long and i'm exploring both program size and data size while definiing what those two concepts are in the first place. so maybe the better structure would be to introduce the concepts by themselves first and then refer to that primordial chapter in the other chapters related to each measure?

also was thinking of restructuring the size chapter to begin with the equation `programs = algorithms + data structures` so that it sets the stage for the discourse on both programs and data.

**Dec-18-2013 17:07 :** Back to guts after a hiatus of 2 exactly months. This time was spent spinning out jmx.js - which was supposed to be a simple "weekend project". Ha!

**Jan-13-2014 17:17 :** Finished up data sizing - almost. Now taking a step back and looking at what I've done so far:

So the story so far is: I started out with "wouldnt it be nice to have a theory for software like we have for physics", created a mindmap of the end state - the overall model, different things that could be measured and so forth; then i presented at barcamp and hareesh gave me the idea to focus on a single measure and try to pin it down to make it concrete. so i started out with size and the notes that i started to write to keep things sane for myself became this book. as i wrote the book it became obvious that the theory cannot be an absolute one. physics is an absolute theory because (within limits of "regular sized objects") there are absolute limits. not so with the mind. thus the theory has become merely a useful one.

typically science works thus: we observe something in the real world and we model it as a theory. the model is an abstraction; so its very focused on certain aspects of the real world and ignores others. however, the net result is that some facts about the real world are captured in the theory. from these initial facts, some rules are proposed, which produce new synthesized facts. Experiments are carried out to check if these synthesized facts are true in the real world. if so, the theory holds and if not, it doesnt.

GUTS is not yet that kind of theory. 

Like the [Dreyfus Model](http://en.wikipedia.org/wiki/Dreyfus_model_of_skill_acquisition), however, it is a useful theory in that it *could* be used to compute size (for now, more in future) in a completely implementation-neutral way.

Actually, now that I've been reading up, GUTS is more correctly named as GUMS - Grand Unified Model of Software. Be that as it may, I'm going to continue to call it guts because:

a) its had this name for a while
b) there is some hope of predictive capability yet :)

For the most impact in terms of convincing people that this theory is useful, however, it needs to have concrete uses. Some that I can think of are:

* Estimating effort of changing an existing application
* Estimating an new app and validating version 1

Both of which are constrained by the lack of an easy way to do the actual measurement of code itself - key point is: how do I count size of code using turings? loc is easy to do, not so much atomic operations. Especially considering it is defined as "compound op is equivalent to the set of simple ops that has the same effect", not "compound op is equivalent to the number of sub expressions its ast reduces to". can the latter be same as the former, tho? need to consider this, and how to express it in the theory. if that's true, then it would be a easier process to instrument an appropriate parser generator to output counts at the language's statement level.

**Jan-16-2014 17:32 :** Now that I have a way to measure data, i should be able to measure true program size as the combo of code size and data size. this will pave the way to account for the "additional size" that the temporary variables required to convert a compound op into a set of simple ops. Then i could compare with the ast tree of the same compound op like so:

		size(set of simple ops replacing compound op) = data_size(new vars) + code_size(simple ops)
		size(ast for compound op) = sum(data_size(simple ops that make up the ast))

Converting both to data size, I hope to prove that:

		size(set of simple ops replacing compound ap) - size(new ops) = size(ast for compound op)

Now we can just use regular asts to calculate size. This makes it a tad easier: as long as we have enough grammars written, languages can be sized using a special treewalker that counts size instead of interpreting or compiling the source.

i did look for a parser generator that has a lot of grammars and the ability to plug into specific points; and so far antlr seems to be the best. i still have to look at the llvm set, tho.

For a web app, the bare minimum languages required are: html, css, js, one server language.

**Jan-17-2014 08:00 :**  Also realized that while the code structures are 2d, their corresponding data structures are not. Should this dichotomy hold?

* Yes: data is inherently stored flat and interpreted into memory structures that are potentially non-flat
* No: regardless of how it is stored, if the conceptual structure has more than one dimension, the data unit should represent that. this also has the happy coincidence of matching with code structures.

I'm tending towards the No. Rewrite of the data size sections are in order, therefore. However, this shows the subjective nature of this theory: i have to explicitly align myself with my previous thoughts so that "everything works out", not that it naturally does.

**Jan-22-2014 08:30 :** While in the midst of installing Java so that i can try out treewalker, I've started a separate rethink of my byte-vs-higher-UOM screed from the top half of data_size.md. It seems a bit contrived, especially when hypenated units are required at higher dimensions. Now reading up on dimensional analysis to see if that can be of help.

**Jan-23-2014 17:14 :** Reading a lot of wiki links:
http://en.wikipedia.org/wiki/Dimensional_analysis
http://en.wikipedia.org/wiki/Dimension_(mathematics_and_physics)
http://en.wikipedia.org/wiki/Inductive_dimension
http://en.wikipedia.org/wiki/Hausdorff_dimension
http://en.wikipedia.org/wiki/Minkowski_dimension
http://en.wikipedia.org/wiki/Fractal_dimension
http://en.wikipedia.org/wiki/Degrees_of_freedom (as a contrast)
http://havlin.biu.ac.il/PS/Dimension%20of%20spatially%20embedded%20networks.pdf - very interesting paper as it measures fractal dimension of networks.
http://en.wikipedia.org/wiki/Quantity_calculus

**Jan-24-2014 17:00 :** Was thinking today morning about container sizes and the units to express them. I currently express them as, for eg, "numbers as list units" which is a very unwieldy way of expressing something. but when we consider containers in real life: there are fixed size containers for sure, but there are also flexible containers that "take the shape of their contents". and while there are transparent containers, there are also "black bag"s or paper packs that you have to open/tear up to see the contents. even if you cannot see the contents, you can size the container as "a brown bag that's a foot wide and deep and 2 feet tall". so why not apply this logic to our data containers? they take the size from their contents and express it as their own size. so we can say "a list 100 tall and 3 numbers+2addresses wide".

This of course brings up the point that it'd be then nice to reduce it further to "a list of size 500 sq teds"

What if we made the theory data-type agnostic? All data is data, so 2 numbers + 2 addresses = 5 pieces of data even if they're of different size. What matters is that they're important to be counted and treated as separate entities. At a lower level of abstraction, it might be required to break an address down into a house number, a street, a zip and so forth, but not at this level.
