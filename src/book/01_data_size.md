<a name="sizing_data"></a>
Sizing up data
--------------

All this while we've been talking about the operations and have pretty much ignored the data that programs are typically associated with. In fact the SSI section had a few variables and I glossed over them on purpose because it was relatively easy to deal with operations by themselves. Now that we have a handle on the operations, however, its time to go back and give data its due.

### What is Data?

First of all, what is data? To go back to the visualizations, data is what flows through the blocks and exercises them. It is the liquid in the Pipes analogy and the marbles in the marble run analogy. It makes the wheels turn and the flip-flop flip (or flop). It is what uses the code. As a definition, therefore:

> Data: Pieces of information used by code.

Some data is input, some is generated by the program itself while it executes and some is output. Static languages pre-allocate some data for efficiency and allocate the rest while running; dynamic languages allocate most of their data during execution. Some data is static, some dynamic; some are scalar, others not. All these facets are ways of classifying data, but the core definition remains true in all.

### The basic structure of data

What is the smallest chunk of information - the single datum? Unlike code, there is no single such chunk; instead we have a collection of "smallest possible in their class" chunks - a number, a character, an amount and so forth; and a language has a finite set of these. The "class" of chunk as I've informally named it, is essentially one piece of Metadata - data about data. Once we start talking about data, it is impossible to avoid talking about metadata - its type or kind, the limits of values and so forth. As definitions, therefore:

> Data: the smallest pieces of different kinds of information the language allows.

> Metadata: Data about the data, eg, its kind or type.

Once we have these primitives, we can start grouping them to form interesting collections:

* List: An ordered collection of data of the same kind. Examples are an array of increasing numbers or a string of characters that form a name.
* Set: An unordered collection of data of the same kind. An example is a collection of poll data.
* Record: A collection of data of different kinds that forms a cohesive whole. An example is an address: it has a street address, typically a house number, a zip and so forth.
* Tree: A hierarchy of data. An example would be a family tree.
* Graph: A set of data that is related to each other in some way; depicted as nodes and relations between nodes. A typical example is a telephone network or a flight map.
* ... and so forth...

More interestingly, each of these collections - or **Compound Data** as I'll call them - can also contain other Compound data as their items. 
> Compound Data: Data that contains other data within itself - simple or compound.

With Compound Data, infinite kinds of data structures can be created using the basic data types.

Two additional kinds of data that we will naturally gravitate towards are References and Links. A reference is a a way to locate a piece of data - a unique identifier to it. A link connects two pieces of data; that is, it is a piece of data that stores a reference to another piece of data. The referred piece could be atomic or compound. In languages that support it, they're typically called pointers or references (although languages like C++ have both and they have different semantics - we will stick to the english meaning, therefore).

> Reference: A datum that locates another piece of data - simple or compound. It is a unique identifier for that piece of data.

> Link: A connection between two pieces of data. Typically it manifests as a reference to the second piece of data stored in the first piece.


### Current notions of data size

We already have a universal unit of measurement for data size - the Byte (or its 1/8th equivalent - the Bit). This unit is machine-independent and in tune with the stored-program concept that all computers are examples of. There is, however, a homogenity that the byte brings to data - essentially everything is a binary number. All other kinds of data - text, pictures, attribute collections are reduced to a single stream of bits; subject to interpretation by an intelligent observer. In the hands of the right interpreter (program or human), binary digits are processed correctly and meaning emerges; in the hands of another, gibberish does. This "data is in the eye of the beholder" nature(sic) is even exploited in some languages to store 2 pieces of information in the same memory - for example, in [C unions](http://tbd/).

Should we care that the byte homogenizes all data? With code, we gave all the primorial operations the same numeric size because we couldnt discern their internal workings any better, but with data they ARE of different sizes. So should we give weight to the fact that meaning emerges above the byte level?

There is another well-known way to measure data size used in the analysis of algorithms' complexity - the Big Omega. This measures the "order of magnitude of the maximum possible size of a data structure when the input contains n items". It produces an imprecise, but sufficiently accurate and useful estimate of the size of a data structure; so it is used routinely to make design and performance decisions by practitioners.

A concept that is otherwise unrelated, but similar in sprit to Big Omega is used [in Personal Privacy circles](http://tbd/). Confusingly, it is also called a Bit. In this context, however, a 'bit' is a chunk of information - it could be your SSN or postal address or some such real-world attribute of a person which can be used to uniquely identify a person. The more number of bits about you are available in public, the easier it is to triangulate your location, preferences, family, etc and gain access to your bank accounts and such protected assets.

The latter two ideas of data seem to be more higher-level than the byte, despite their general lack of accuracy. They do more things: the second measures rate of change of data size well and the third allows for rich expression of complex data and their types. 

So, which one should we pick? In keeping with our goal to arrive at natural measures, our data size measure should:

* Used the structure of the data **AS DATA**,
* Was language agnostic,
* Was indeed measurable,
* Was extensible to define other properties of data with,
* And yet was easy for humans to understand and use to understand a lot of information in one go.

The byte is certainly language agnostic, measurable and somewhat easy for humans to understand because it globs all data into a simple number. However, it _does_ whitewash all data into binary digits. The byte is like the concept of "text": anything could be treated as text - prose, poetry, todo lists, essays and so forth; we gain meaning from the text only when we interpret it as being one of those specific kinds of things.

The other two ways of thinking of data, however, are measurable and language agnostic; but more importantly they are extensive and easy for humans to understand BECAUSE they treat data as data. 

What if we were to use something closer to the second and third ideas of size? We'd have to settle on a basic set of primitive data types and another set of compound data types. The latter will be sized in terms of the former. Since that still leaves us with the problem of sizing the primitives themselves, let's keep the byte as a measure for them. For example,

		if   1 unit = 2 bytes
		if   datatype A = 1 unit  => datatype A = 2 bytes
		then datatype B = 2 units => datatype B = 2 x datatype A & datatype B = 4 bytes

Why would we take this two step process instead of just going with byte? Quite a few reasons:

1. It treats data as DATA
2. The conversion from data units to bytes hides the word length of the processor. This in effect abstracts out that implementation detail while stil making it available if required.
3. It allows for discourse in terms of the application domain. This might be useful if the software is not yet built and the domain models are hazy enough that a mapping to persistent object is premature.

Let's try this out for size (sic) and see how things go.

### Sizing up well known data types

Let's try sizing up some well known and routine data types using this new, yet unnamed size measure.

TODO: TALK ABOUT DATA SIZE BOTH STATIC AND DYNAMIC.
CIRCLE BACK TO HOW CODE SIZE IS DATA SIZE AND VV BCOS CODE SIZE IS MEASURING THE SIZE OF A GRAPH DATA STRUCTURE.
