<a name="sizing_data"></a>
Sizing up data
--------------

All this while we've been talking about the operations and have pretty much ignored the data that programs are typically associated with. In fact the SSI section had a few variables and I glossed over them on purpose because it was relatively easy to deal with operations by themselves. Now that we have a handle on the operations, however, its time to go back and give data its due.

### What is Data?

First of all, what is data? To go back to the visualizations, data is what flows through the blocks and exercises them. It is the liquid in the Pipes analogy and the marbles in the marble run analogy. It makes the wheels turn and the flip-flop flip (or flop). It is what uses the code. As a definition, therefore:

> Data: Pieces of information used by code.

Some data is input, some is generated by the program itself while it executes and some is output. Static languages pre-allocate some data for efficiency and allocate the rest while running; dynamic languages allocate most of their data during execution. Some data is static, some dynamic; some are scalar, others not. All these facets are ways of classifying data, but the core definition remains true in all.

### The basic structure of data

What is the smallest chunk of information - the single datum? Unlike code, there is no single such chunk; instead we have a collection of "smallest possible in their class" chunks - a number, a character, an amount and so forth; and a language has a finite set of these. The "class of chunk" as I've informally named it, is essentially one piece of Metadata - data about data; and once we start talking about data, it is impossible to avoid talking about metadata - its type or kind, the limits of values and so forth. As definitions, therefore:

> Data: the smallest pieces of different kinds of information the language allows.

> Metadata: Data about the data, eg, its kind or type.

Once we have these primitives, we can start grouping them to form interesting collections:

* List: An ordered collection of data of the same kind. Examples are an array of increasing numbers or a string of characters that form a name.
* Set: An unordered collection of data of the same kind. An example is a collection of poll data.
* Record: A collection of data of different kinds that forms a cohesive whole. An example is an address: it has a street address, typically a house or apartment number, a zip and so forth.
* Tree: A hierarchy of data. An example would be a family tree.
* Graph: A set of data that is related to each other in some way; depicted as nodes and relations between nodes. A typical example is a telephone network or a flight map.
* ... and so forth...

More interestingly, each of these collections - or **Compound Data** as I'll call them - can also contain other Compound data as their items. 
> Compound Data: Data that contains other data within itself - simple or compound.

With Compound Data, infinite kinds of data structures can be created using the basic data types.

Two additional kinds of data that we will naturally gravitate towards are References and Links. A reference is a a way to locate a piece of data - a unique identifier to it. A link connects two pieces of data; that is, it is a piece of data that stores a reference to another piece of data. The referred piece could be atomic or compound. In languages that support it, they're typically called pointers or references (although languages like C++ have both and they have different semantics - we will stick to the english meaning, therefore).

> Reference: A datum that locates another piece of data - simple or compound. It is a unique identifier for that piece of data.

> Link: A connection between two pieces of data. Typically it manifests as a reference to the second piece of data stored in the first piece.

To summarize:

* Data are pieces of information used by code
* Metadata is data about data
* There is no single primoridal kind of data
* Instead, we pick a set of data types as primitive and assign them sizes relative to each other.
* Once we have primitives, we can start forming collections of them. The sizes of collections are related to the size of their contents.
* Two special kinds of data are references and links. References are unique identifiers to specific pieces of data and links point one piece of data to another by storing references.
* We can thus calculate the size of all the data used in code.

### Current notions of data size

We already have a universal unit of measurement for data size - the Byte (or its 1/8th equivalent - the Bit). This unit is machine-independent and in tune with the stored-program concept that all computers are examples of. There is, however, a homogenity that the byte brings to data - essentially everything is a binary number. All other kinds of data - text, pictures, attribute collections are reduced to a single stream of bits; subject to interpretation by an intelligent observer. In the hands of the right interpreter (program or human), binary digits are processed correctly and meaning emerges; in the hands of another, gibberish does. This "data is in the eye of the beholder" nature(sic) is even exploited in some languages to store 2 pieces of information in the same memory - for example, in [C unions](http://tbd/).

Should we care that the byte homogenizes all data? With code, we gave all the primorial operations the same numeric size because we couldnt discern their internal workings any better, but with data they ARE of different sizes. So should we give weight to the fact that meaning emerges above the byte level?

There is another well-known way to measure data size used in the analysis of algorithms' complexity - the Big Omega. This measures the "order of magnitude of the maximum possible size of a data structure when the input contains n items". It produces an imprecise, but sufficiently accurate and useful estimate of the size of a data structure; so it is used routinely to make design and performance decisions by practitioners.

A concept that is otherwise unrelated, but similar in sprit to Big Omega is used [in Personal Privacy circles](http://tbd/). Confusingly, it is also called a Bit. In this context, however, a 'bit' is a chunk of information - it could be your SSN or postal address or some such real-world attribute of a person which can be used to uniquely identify a person. The more number of bits about you are available in public, the easier it is to triangulate your location, preferences, family, etc and gain access to your bank accounts and such protected assets.

The latter two ideas of data seem to be more higher-level than the byte, despite their general lack of accuracy. They do more things: the second measures rate of change of data size well and the third allows for rich expression of complex data and their types. 

So, which one should we pick? In keeping with our goal to arrive at natural measures, our data size measure should:

* Used the structure of the data **AS DATA**,
* Was language agnostic,
* Was indeed measurable,
* Was extensible to define other properties of data with,
* And yet was easy for humans to understand and use to understand a lot of information in one go.

The byte is certainly language agnostic, measurable and easy for humans to understand because it globs all data into a simple number. However, it _does_ whitewash all data into binary digits. The byte is like the concept of "text": anything could be treated as text - prose, poetry, todo lists, essays and so forth; we gain meaning from the text only when we interpret it as being one of those specific kinds of things.

The other two ways of thinking of data, however, are measurable and language agnostic; but more importantly they are extensive and easy for humans to understand BECAUSE they treat data as data.

What if we were to use something closer to the second and third ideas of size? We'd have to settle on a basic set of primitive data types and another set of compound data types. The latter will be sized in terms of the former. Since that still leaves us with the problem of sizing the primitives themselves, let's keep the byte as a measure for them. So, for example,

		if   1 unit = 2 bytes, and
		if   datatype A = 1 unit  => datatype A = 2 bytes
		then datatype B = 2 units => datatype B = 4 bytes
		                             datatype B also happens to be 2 x datatype A (not that any meaning should be attached to that fact)

Why would we take this two step process instead of just going with byte? Couple of reasons:

1. It treats data as DATA
2. It allows for discourse in terms of the application domain. This might be useful if the software is not yet built and the domain models are hazy enough that a mapping to persistent object is premature.

### Sizing up well known primitives

Let's try sizing up some well known and routine data types using this new, yet unnamed size measure.


#### Numbers

Since numbers are what computers do best, let's start with them. Immediately we have to be aware of the different kinds of numbers: integers, fractions or rational numbers, real numbers, complex numbers and so forth. What can we say about their size when treated as pure data? Not much. We could possibly enumerate how many of them we might need for a particular application with some domain analysis; and possibly - with some more effort - set bounds on the range of values that these numbers might take in that domain.

There is an implementation angle to this, however: all computers are finite machines; they do not have infinite storage. The "number line" that a computer can express is clearly a subset of the conceptual endless-on-both-sides number line that we study in school. This - the largest number that a computer can express intinsically - is usually called its"Word Length" and is usually expressed in bytes.

Numbers that are smaller than the word length are readily processed by the machine; operations such as addition and multiplication are available in the instruction set for them. Numbers that are beyond the range, however, need special handling in terms of libraries or special instructions, with resultant reduction in speed.

Regardless, at a human (natural?) level, these are all numbers. We could choose, therefore to equate each such number to one data unit, but we have to qualify it with the "type", so we could say, for example:

	An list with 100 numbers has a size of 100 number units
	or in formula:

	1 number = 1 number unit
	Thus, size(list(100 numbers)) = 100 number units
                                                                                         --(A)

Assuming that these numbers were in the range `+/- [(2^32)-1]`, that list of 100 numbers would take up a different size based on the word length of the actual machine when implemented. For example, here are three computers, A, B & C:

	Computer A's word length = 4 bytes => size(list) = 100 number units = 100 words = 400 bytes
	Computer B's word length = 2 bytes => size(list) = 100 number units = 200 words = 400 bytes
	Computer C's word length = 8 bytes => size(list) = 100 number units = 100 words = 800 bytes
                                                                                         --(B)
A few points to note here:

* The "number units" measure retained its size regardless of changes in implementation machine.
** In contrast, Word length did remain constant or even linear: increasing the word length in C didnt increase the total word size for the list in the same way as decreasing the word length did.
** Similarly, bytes as a measure of data size didnt correlate well either; despite halving the word length between A & B, the size in bytes remained the same; but that didnt hold when moving to C.
** This is, of course, a direct consequence of using word boundaries, but it does underline the disadvantage with using bytes as a unit of measure to express the natural size of data.
* We cheated a bit in that "number units" should actually be more accurately termed "numbers between -2^32 and +2^32". To avoid confusion between english (or pure mathematical) meanings of words used to qualify data types, we should probably use a name like "num32". This obviously is reminicient of the similarly named types in languages like C, but it does remove ambiguity. When the domain is sufficiently well-defined, we could probably dispense with this exactness and assume that "everyone knows a number is under 2^32".

We could apply the same sort of logic for other kinds of numbers; for example:

	1 real number = 1 real number unit
	1 complex number = 1 complex number unit
	                                                                                     --(C)

.. and so forth. We could even define "number" to include everything from integers to complex numbers and call that a single number unit. Again, the advantage of this "intentional blindness" to the implementation details is that it allows us to think in terms of the domain and not the machine. As demonstrated above, all such abstract units can be readily converted into bytes using the machine's word length as the yardstick when we need to compare sizes in physical terms.

#### Text

Text as we humans use it, is the name for a string of letters or characters. Quite naturally, that can be seen as a compound data item - an ordered list of letters. So we could define, for example:

	size(1 letter or character) = 1 text unit
	
	Regular text is a list or array of characters. 
	Thus size(regular text) = size(character array)
	                                                                                     --(D)

As mentioned above, computers handle text by encoding text into a lookup table and storing the indexes as stand-ins for the real textual values. A common encoding is ASCII, which uses 1 byte per character; another is UTF-8 which also takes one byte per character but allows for expression of more human languages than ASCII. Regardless of the encoding used, text is essentially numbers within the computer and have the same inherent disadvantage of varying with the physical machine's word length and the encoding used. 

The machine-agnostic "text unit" measure does not have this disadvantage, but it still needs the same warning about latent meaning in the terms used for the data type.

#### Logical (True/False) values

True/False values are pretty straightforward to think of in terms of a data unit - each one is one logical data unit. Comparisons between this logical data unit and bytes follow similar lines as the discussion for numbers:

	Computer A's word length = 4 bytes => size(boolean value) = 1 bool unit = 4 bytes
	Computer B's word length = 2 bytes => size(boolean value) = 1 bool unit = 2 bytes
	Computer C's word length = 8 bytes => size(boolean value) = 1 bool unit = 8 bytes
                                                                                         --(E)

There are, of course compound data types like bit vectors which store the extra bytes to store additional boolean values and use boolean algebra to update or read this information. If such a data structure were required in the domain of the application, they would still be counted as multiples of a primitive boolean unit instead of fractions of a byte. That is, 

	Computer A's word length = 4 bytes => size(list of 100 bools) = 100 bool units = 4 words (13 up to 16) = 13 bytes (12.5 rounded up) 
	Computer B's word length = 2 bytes => size(list of 100 bools) = 100 bool units = 7 words (13 up to 14) = 13 bytes (12.5 rounded up) 
	Computer C's word length = 8 bytes => size(list of 100 bools) = 100 bool units = 2 words (13 up to 16) = 13 bytes (12.5 rounded up) 
                                                                                         --(F)

#### Enumerations

Enumerations have some natural parallels like days of week, months in a year and so forth, but they are really a programmer's convenience. In terms of natural units, they would be another integer type with the range of values being constrained to the allowed limits and specific one-to-one mappings of name to value. Thus, there is no single "Enumeration type", only specific instances of the pattern like a "days of the week unit" that is allowed the values "Mon, Tue, Wed, Thu, Fri, Sat, Sun". In the realm of the data unit that we're talking about, the exact numeric value that each of these are assigned to (eg, Mon=0 vs Mon=1) is of not much consequence.

#### References and Links

References do not have a direct natural complement - the natural way to refer to something exclusively is to enumerate all the attributes that make it unique. This is, of course, cumbersome as the reference becomes almost as large as the thing itself, if not larger. Natural language appreciates compaction as much as computer representation does, however, so we HAVE come up with ways to refer to things and people without having to describe their every difference from other things and people, two examples being SSNs and Driver's Licence numbers. These are essentially real-world references; they do not actually describe the person, but once assigned, they can be used to refer to (and more importantly retrive information about) a person exclusively.

When natural references are available, creating units for them is straightforward:

	size(1 ref)  = 1 ref unit
	size(1 link) = 1 ref unit
	                                                                                     --(G)

That is, the size of a link to something is the same as the size of the reference itself, simply because the link is another reference!

When natural references are not available, however, it is possible to create artificial ones: a particular piece of data could be refered to by its position in the overall collection of data - "the 5th integer", "the 13th player" and so forth; or a particular piece of data could be referred to by how its attributes were recorded in some system of record - "person found in ledger 13, folio 345, line 56" or "psalm 3:16". Whether each such component of the reference should be accounted for separately is a domain concern, but the key point is that they can be considered as separate from the thing that they describe and they have some finite, enumerable size and therefore can be measured.

There is a strong similarity between this latter scheme from the real world and how references and links are represented in computers. References and links in computers are essentially memory addresses. In plain english, they are positional counters from the first addressible memory location. Once assigned (at least for the lifetime of the running program), they are uniquely addressible and allow access to retrieve information about a specific object.

The key question, therefore, is not "How do we ascribe a size to references and links?", but rather: "Will there be links to a particular object and should those links be references?" If the answers to the questions are "Yes and Yes", then we should use (G) above to count their sizes. Where artifical references (or memory addresses) are used, we can call those out as a separate "memory reference/link unit".

### Sizing up well known compound data

#### Sizing up Arrays & Lists

#### Sizing up Maps

#### Sizing up Trees

#### Sizing up Graphs

STOPPED HERE Jan 6 AM

TODO: TALK ABOUT DATA SIZE BOTH STATIC AND DYNAMIC.
CIRCLE BACK TO HOW CODE SIZE IS DATA SIZE AND VV BCOS CODE SIZE IS MEASURING THE SIZE OF A GRAPH DATA STRUCTURE.
