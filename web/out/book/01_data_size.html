<html>
<head>
  <title></title>
  <link rel="stylesheet" type="text/css" href="css/guts.css"></link>
</head>
<body>

<p><a name="sizing_data"></a></p>
<h2>Sizing up data</h2>
<p>All this while we&#39;ve been talking about the operations and have pretty much ignored the data that programs are typically associated with. In fact the SSI section had a few variables and I glossed over them on purpose because it was relatively easy to deal with operations by themselves. Now that we have a handle on the operations, however, its time to go back and give data its due.</p>
<h3>What is Data?</h3>
<p>First of all, what is data? To go back to the visualizations, data is what flows through the blocks and exercises them. It is the liquid in the Pipes analogy and the marbles in the marble run analogy. It makes the wheels turn and the flip-flop flip (or flop). It is what uses the code. As a definition, therefore:</p>
<blockquote>
<p>Data: Pieces of information used by code.</p>
</blockquote>
<p>Some data is input, some is generated by the program itself while it executes and some is output. Static languages pre-allocate some data for efficiency and allocate the rest while running; dynamic languages allocate most of their data during execution. Some data is static, some dynamic; some are scalar, others not. All these facets are ways of classifying data, but the core definition remains true in all.</p>
<h3>The basic structure of data</h3>
<p>What is the smallest chunk of information - the single datum? Unlike code, there is no single such chunk; instead we have a collection of &quot;smallest possible in their class&quot; chunks - a number, a character, an amount and so forth; and a language has a finite set of these. The &quot;class of chunk&quot; as I&#39;ve informally named it, is essentially one piece of Metadata - data about data; and once we start talking about data, it is impossible to avoid talking about metadata - its type or kind, the limits of values and so forth. As definitions, therefore:</p>
<blockquote>
<p>Data: the smallest pieces of different kinds of information the language allows.</p>
<p>Metadata: Data about the data, eg, its kind or type.</p>
</blockquote>
<p>Once we have these primitives, we can start grouping them to form interesting collections:</p>
<ul>
<li>List: An ordered collection of data of the same kind. Examples are an array of increasing numbers or a string of characters that form a name.</li>
<li>Set: An unordered collection of data of the same kind. An example is a collection of poll data.</li>
<li>Record: A collection of data of different kinds that forms a cohesive whole. An example is an address: it has a street address, typically a house or apartment number, a zip and so forth.</li>
<li>Tree: A hierarchy of data. An example would be a family tree.</li>
<li>Graph: A set of data that is related to each other in some way; depicted as nodes and relations between nodes. A typical example is a telephone network or a flight map.</li>
<li>... and so forth...</li>
</ul>
<p>More interestingly, each of these collections - or <strong>Compound Data</strong> as I&#39;ll call them - can also contain other Compound data as their items. </p>
<blockquote>
<p>Compound Data: Data that contains other data within itself - simple or compound.</p>
</blockquote>
<p>With Compound Data, infinite kinds of data structures can be created using the basic data types.</p>
<p>Two additional kinds of data that we will naturally gravitate towards are References and Links. A reference is a a way to locate a piece of data - a unique identifier to it. A link connects two pieces of data; that is, it is a piece of data that stores a reference to another piece of data. The referred piece could be atomic or compound. In languages that support it, they&#39;re typically called pointers or references (although languages like C++ have both and they have different semantics - we will stick to the english meaning, therefore).</p>
<blockquote>
<p>Reference: A datum that locates another piece of data - simple or compound. It is a unique identifier for that piece of data.</p>
<p>Link: A connection between two pieces of data. Typically it manifests as a reference to the second piece of data stored in the first piece.</p>
</blockquote>
<p>To summarize:</p>
<ul>
<li>Data are pieces of information used by code</li>
<li>Metadata is data about data</li>
<li>There is no single primoridal kind of data</li>
<li>Instead, we pick a set of data types as primitive and assign them sizes relative to each other.</li>
<li>Once we have primitives, we can start forming collections of them. The sizes of collections are related to the size of their contents.</li>
<li>Two special kinds of data are references and links. References are unique identifiers to specific pieces of data and links point one piece of data to another by storing references.</li>
<li>We can thus calculate the size of all the data used in code.</li>
</ul>
<h3>Current notions of data size</h3>
<p>We already have a universal unit of measurement for data size - the Byte (or its 1/8th equivalent - the Bit). This unit is machine-independent and in tune with the stored-program concept that all computers are examples of. There is, however, a homogenity that the byte brings to data - essentially everything is a binary number. All other kinds of data - text, pictures, attribute collections are reduced to a single stream of bits; subject to interpretation by an intelligent observer. In the hands of the right interpreter (program or human), binary digits are processed correctly and meaning emerges; in the hands of another, gibberish does. This &quot;data is in the eye of the beholder&quot; nature(sic) is even exploited in some languages to store 2 pieces of information in the same memory - for example, in <a href="http://tbd/">C unions</a>.</p>
<p>Should we care that the byte homogenizes all data? With code, we gave all the primorial operations the same numeric size because we couldnt discern their internal workings any better, but with data they ARE of different sizes. So should we give weight to the fact that meaning emerges above the byte level?</p>
<p>There is another well-known way to measure data size used in the analysis of algorithms&#39; complexity - the Big Omega. This measures the &quot;order of magnitude of the maximum possible size of a data structure when the input contains n items&quot;. It produces an imprecise, but sufficiently accurate and useful estimate of the size of a data structure; so it is used routinely to make design and performance decisions by practitioners.</p>
<p>A concept that is otherwise unrelated, but similar in sprit to Big Omega is used <a href="http://tbd/">in Personal Privacy circles</a>. Confusingly, it is also called a Bit. In this context, however, a &#39;bit&#39; is a chunk of information - it could be your SSN or postal address or some such real-world attribute of a person which can be used to uniquely identify a person. The more number of bits about you are available in public, the easier it is to triangulate your location, preferences, family, etc and gain access to your bank accounts and such protected assets.</p>
<p>The latter two ideas of data seem to be more higher-level than the byte, despite their general lack of accuracy. They do more things: the second measures rate of change of data size well and the third allows for rich expression of complex data and their types. </p>
<p>So, which one should we pick? In keeping with our goal to arrive at natural measures, our data size measure should:</p>
<ul>
<li>Used the structure of the data <strong>AS DATA</strong>,</li>
<li>Was language agnostic,</li>
<li>Was indeed measurable,</li>
<li>Was extensible to define other properties of data with,</li>
<li>And yet was easy for humans to understand and use to understand a lot of information in one go.</li>
</ul>
<p>The byte is certainly language agnostic, measurable and easy for humans to understand because it globs all data into a simple number. However, it <em>does</em> whitewash all data into binary digits. The byte is like the concept of &quot;text&quot;: anything could be treated as text - prose, poetry, todo lists, essays and so forth; we gain meaning from the text only when we interpret it as being one of those specific kinds of things.</p>
<p>The other two ways of thinking of data, however, are measurable and language agnostic; but more importantly they are extensive and easy for humans to understand BECAUSE they treat data as data.</p>
<p>What if we were to use something closer to the second and third ideas of size? We&#39;d have to settle on a basic set of primitive data types and another set of compound data types. The latter will be sized in terms of the former. Since that still leaves us with the problem of sizing the primitives themselves, let&#39;s keep the byte as a measure for them. So, for example,</p>
<pre><code>    if   1 unit = 2 bytes, and
    if   datatype A = 1 unit  =&gt; datatype A = 2 bytes
    then datatype B = 2 units =&gt; datatype B = 4 bytes
                                 datatype B also happens to be 2 x datatype A (not that any meaning should be attached to that fact)</code></pre>
<p>Why would we take this two step process instead of just going with byte? Couple of reasons:</p>
<ol>
<li>It treats data as DATA</li>
<li>It allows for discourse in terms of the application domain. This might be useful if the software is not yet built and the domain models are hazy enough that a mapping to persistent object is premature.</li>
</ol>
<h3>Sizing up well known primitives</h3>
<p>Let&#39;s try sizing up some well known and routine data types using this new, yet unnamed size measure.</p>
<h4>Numbers</h4>
<p>Since numbers are what computers do best, let&#39;s start with them. Immediately we have to be aware of the different kinds of numbers: integers, fractions or rational numbers, real numbers, complex numbers and so forth. What can we say about their size when treated as pure data? Not much. We could possibly enumerate how many of them we might need for a particular application with some domain analysis; and possibly - with some more effort - set bounds on the range of values that these numbers might take in that domain.</p>
<p>There is an implementation angle to this, however: all computers are finite machines; they do not have infinite storage. The &quot;number line&quot; that a computer can express is clearly a subset of the conceptual endless-on-both-sides number line that we study in school. This - the largest number that a computer can express intinsically - is usually called its&quot;Word Length&quot; and is usually expressed in bytes.</p>
<p>Numbers that are smaller than the word length are readily processed by the machine; operations such as addition and multiplication are available in the instruction set for them. Numbers that are beyond the range, however, need special handling in terms of libraries or special instructions, with resultant reduction in speed.</p>
<p>Regardless, at a human (natural?) level, these are all numbers. We could choose, therefore to equate each such number to one data unit, but we have to qualify it with the &quot;type&quot;, so we could say, for example:</p>
<pre><code>An list with 100 numbers has a size of 100 number units
or in formula:

1 number = 1 number unit
Thus, size(list(100 numbers)) = 100 number units
                                                                                     --(A)</code></pre>
<p>Assuming that these numbers were in the range <code>+/- [(2^32)-1]</code>, that list of 100 numbers would take up a different size based on the word length of the actual machine when implemented. For example, here are three computers, A, B &amp; C:</p>
<pre><code>Computer A&#39;s word length = 4 bytes =&gt; size(list) = 100 number units = 100 words = 400 bytes
Computer B&#39;s word length = 2 bytes =&gt; size(list) = 100 number units = 200 words = 400 bytes
Computer C&#39;s word length = 8 bytes =&gt; size(list) = 100 number units = 100 words = 800 bytes
                                                                                     --(B)</code></pre>
<p>A few points to note here:</p>
<ul>
<li>The &quot;number units&quot; measure retained its size regardless of changes in implementation machine.
<strong> In contrast, Word length did remain constant or even linear: increasing the word length in C didnt increase the total word size for the list in the same way as decreasing the word length did.
</strong> Similarly, bytes as a measure of data size didnt correlate well either; despite halving the word length between A &amp; B, the size in bytes remained the same; but that didnt hold when moving to C.
** This is, of course, a direct consequence of using word boundaries, but it does underline the disadvantage with using bytes as a unit of measure to express the natural size of data.</li>
<li>We cheated a bit in that &quot;number units&quot; should actually be more accurately termed &quot;numbers between -2^32 and +2^32&quot;. To avoid confusion between english (or pure mathematical) meanings of words used to qualify data types, we should probably use a name like &quot;num32&quot;. This obviously is reminicient of the similarly named types in languages like C, but it does remove ambiguity. When the domain is sufficiently well-defined, we could probably dispense with this exactness and assume that &quot;everyone knows a number is under 2^32&quot;.</li>
</ul>
<p>We could apply the same sort of logic for other kinds of numbers; for example:</p>
<pre><code>1 real number = 1 real number unit
1 complex number = 1 complex number unit
                                                                                     --(C)</code></pre>
<p>.. and so forth. We could even define &quot;number&quot; to include everything from integers to complex numbers and call that a single number unit. Again, the advantage of this &quot;intentional blindness&quot; to the implementation details is that it allows us to think in terms of the domain and not the machine. As demonstrated above, all such abstract units can be readily converted into bytes using the machine&#39;s word length as the yardstick when we need to compare sizes in physical terms.</p>
<h4>Text</h4>
<p>Text as we humans use it, is the name for a string of letters or characters. Quite naturally, that can be seen as a compound data item - an ordered list of letters. So we could define, for example:</p>
<pre><code>size(1 letter or character) = 1 text unit

Regular text is a list or array of characters. 
Thus size(regular text) = size(character array)
                                                                                     --(D)</code></pre>
<p>As mentioned above, computers handle text by encoding text into a lookup table and storing the indexes as stand-ins for the real textual values. A common encoding is ASCII, which uses 1 byte per character; another is UTF-8 which also takes one byte per character but allows for expression of more human languages than ASCII. Regardless of the encoding used, text is essentially numbers within the computer and have the same inherent disadvantage of varying with the physical machine&#39;s word length and the encoding used. </p>
<p>The machine-agnostic &quot;text unit&quot; measure does not have this disadvantage, but it still needs the same warning about latent meaning in the terms used for the data type.</p>
<h4>Logical (True/False) values</h4>
<p>True/False values are pretty straightforward to think of in terms of a data unit - each one is one logical data unit. Comparisons between this logical data unit and bytes follow similar lines as the discussion for numbers:</p>
<pre><code>Computer A&#39;s word length = 4 bytes =&gt; size(boolean value) = 1 bool unit = 4 bytes
Computer B&#39;s word length = 2 bytes =&gt; size(boolean value) = 1 bool unit = 2 bytes
Computer C&#39;s word length = 8 bytes =&gt; size(boolean value) = 1 bool unit = 8 bytes
                                                                                     --(E)</code></pre>
<p>There are, of course compound data types like bit vectors which store the extra bytes to store additional boolean values and use boolean algebra to update or read this information. If such a data structure were required in the domain of the application, they would still be counted as multiples of a primitive boolean unit instead of fractions of a byte. That is, </p>
<pre><code>Computer A&#39;s word length = 4 bytes =&gt; size(list of 100 bools) = 100 bool units = 4 words (13 up to 16) = 13 bytes (12.5 rounded up) 
Computer B&#39;s word length = 2 bytes =&gt; size(list of 100 bools) = 100 bool units = 7 words (13 up to 14) = 13 bytes (12.5 rounded up) 
Computer C&#39;s word length = 8 bytes =&gt; size(list of 100 bools) = 100 bool units = 2 words (13 up to 16) = 13 bytes (12.5 rounded up) 
                                                                                     --(F)</code></pre>
<h4>Enumerations</h4>
<p>Enumerations have some natural parallels like days of week, months in a year and so forth, but they are really a programmer&#39;s convenience. In terms of natural units, they would be another integer type with the range of values being constrained to the allowed limits and specific one-to-one mappings of name to value. Thus, there is no single &quot;Enumeration type&quot;, only specific instances of the pattern like a &quot;days of the week unit&quot; that is allowed the values &quot;Mon, Tue, Wed, Thu, Fri, Sat, Sun&quot;. In the realm of the data unit that we&#39;re talking about, the exact numeric value that each of these are assigned to (eg, Mon=0 vs Mon=1) is of not much consequence.</p>
<h4>References and Links</h4>
<p>References do not have a direct natural complement - the natural way to refer to something exclusively is to enumerate all the attributes that make it unique. This is, of course, cumbersome as the reference becomes almost as large as the thing itself, if not larger. Natural language appreciates compaction as much as computer representation does, however, so we HAVE come up with ways to refer to things and people without having to describe their every difference from other things and people, two examples being SSNs and Driver&#39;s Licence numbers. These are essentially real-world references; they do not actually describe the person, but once assigned, they can be used to refer to (and more importantly retrive information about) a person exclusively.</p>
<p>When natural references are available, creating units for them is straightforward:</p>
<pre><code>size(1 ref)  = 1 ref unit
size(1 link) = 1 ref unit
                                                                                     --(G)</code></pre>
<p>That is, the size of a link to something is the same as the size of the reference itself, simply because the link is another reference!</p>
<p>When natural references are not available, however, it is possible to create artificial ones: a particular piece of data could be refered to by its position in the overall collection of data - &quot;the 5th integer&quot;, &quot;the 13th player&quot; and so forth; or a particular piece of data could be referred to by how its attributes were recorded in some system of record - &quot;person found in ledger 13, folio 345, line 56&quot; or &quot;psalm 3:16&quot;. Whether each such component of the reference should be accounted for separately is a domain concern, but the key points are that they can be considered as being separate from the thing that they describe and that they have some finite, enumerable size and therefore can be measured.</p>
<p>There is a strong similarity between this latter scheme from the real world and how references and links are represented in computers. References and links in computers are essentially memory addresses. In plain english, they are positional counters from the first addressible memory location. Once assigned (at least for the lifetime of the running program), they are uniquely addressible and allow access to retrieve information about a specific object.</p>
<p>The key question, therefore, is not &quot;How do we ascribe a size to references and links?&quot;, but rather: &quot;Will there be links to a particular object and should those links be references?&quot; If the answers to the questions are &quot;Yes and Yes&quot;, then we should use (G) above to count their sizes. Where artifical references (or memory addresses) are used, we can call those out as a separate &quot;memory reference/link unit&quot;.</p>
<h3>Sizing up Compound data</h3>
<p>By definition, compound data is a collection of primitive data items that together form a cohesive whole. As such the size of compound data is the aggregate of sizes of the primitive data items contained in it. If there are data items that are not specific to any individual child item, but pertains to the aggregate itself, these are still (from a size perspective) considered to be contained in that that aggregate as well. Thus:</p>
<pre><code>size(compound data) = size(compound data attributes) + size(contained itmes)         --(H)</code></pre>
<p>Let&#39;s usse this formula on some of the often-used/well-known compound data types.</p>
<h4>Sizing up Lists</h4>
<p>From a natural standpoint, lists are pretty straightforward. They are collections (usually ordered) of things of the same kind. So the default sizing for an list would be:</p>
<pre><code>size(list) = sum(size(list item))                                                    --(I)</code></pre>
<p>Their representation in computers, however, vary depending on the implementation chosen: if the total number of items and the size of each item is small, contiguous memory could be allocated to the list; if the sizes are huge, references could be stored instead of the item itself; if the items are to be accessed in random order vs sequential order, the implmentation might be a contiguous array of memory vs linked lists; and so forth. How do we reconcile all these differences so that the natural size that we&#39;re evolving into actually makes the most sense? </p>
<p>The answer lies in (I). Regardless of the implementation, the core of size of a list MUST be a sum of the sizes of the items in it. Where the items are not stored as-is, their contribution to the array&#39;s size is essentially the size of the references stored in it, not the size of the item itself because that size should be counted elsewhere.</p>
<p>When the implementation uses additional storage outside of each item, for example to store the size of the list in a &quot;length&quot; attribute or store a head pointer, we should account for it using (H):</p>
<pre><code>size(list) = size(list attributes) + sum(size(list item))                            --(J)</code></pre>
<p>While this is a simple enough equation, each term needs some discussion on how to apply them.</p>
<h4>Discussion: list attributes</h4>
<p>However, we should apply this overhead ONLY when the domain itself has such list-level attributes, not when the implementation alone does. For example, a list of students in a class and the class strength are prime candidates for representation as a list. The &quot;class strength&quot; attribute is an independently recognizable attribute in the domain. An application written to manage schools SHOULD represent this attribute directly. A display of this list might require pagination and each such subset could be considered a list on its own. Its length is most probably controlled by a configuration value, but should NOT be represented in the sizing because it is an attribute created due to the implementation.</p>
<p>All of this parallels a similar discussion on code size where the &quot;level of abstraction&quot; has a bearing on how we count size. When sizing data, therefore, we should determine the domain up-front and all sizes should be stated as relative to it. To go back to the previous example, therefore:</p>
<ol>
<li>If our domain is an application that manages schools, the list of students in a class would have a list attribute called &quot;class strength&quot; and be sized with this addition, but each page of the display of this list would be sized as in (G).</li>
<li>If our domain is a library of array management functions that the application to manage schools is one client of, we could expose two types of arrays: one with a length attribute and one without. Both would use (I) with list attributes = 0 units for the latter. Note that in this case we might just be using bytes as our unit of measure, especially if the library is very specific to a processor or technology stack.</li>
</ol>
<p>Of course, this is fertile ground for leaning either way - too abstract into the domain end of the spectrum and you can artificially reduce the size; to specific into the implementation domain and you can make the size so specific that it is useless as a generic measure. This issue, however, is inherent with using relative bases. This theory is intended to be used mostly at the level of #1 above and less at #2 because the latter case is not very &quot;natural&quot; by definition. However, it does not preclude application of the theory at that level. The best course would be to reach an agreement (by stating the meanings of each type) amongst the audience of discourse on the level of abstraction.</p>
<h5>Discussion: List items</h5>
<p>What can we say about the <code>sum(size(list item))</code> term? The first thing that stands out is that each list item has a unit attributable to it (eg, a list of numbers would be of type &quot;number unit&quot; and so forth), but the list itself doesnt have a unit of its own.</p>
<p>What it does, however, is provide a container to group data that might otherwise not be. The one type-agnostic measure of size that a list has, therefore, is the count of items in it. Indeed, (J) can be rewritten as:</p>
<pre><code>size(list) = size(list attributes) + size(list item) x N
             where N is the count of list items                                      --(J1)</code></pre>
<p>Applied to a list of 10 grades that are of type &quot;real number unit&quot; and no metadata, therefore,</p>
<pre><code>size(list of grades) = 0 + size(real number unit) x 10 list slot units
                     = 10 real number-list units</code></pre>
<p>A list, therefore is long AND wide - it has two dimensions. Since the data items may be of different types, the squared unit is represented as a hyphenated combination of both the type of the list item and the fact that its in a list. </p>
<p>What would happen if there were list attributes, however? These attributes are part of the list, but not part of the collection of items in the list - they could be of a type completely different from the contained data, for instance. Thus, a list essentially becomes a record containing list attributes and list items. That is,</p>
<pre><code>size(list) = size(record(list attrs, list items))                                    --(J2)</code></pre>
<p>(J1) and (J2) can be reconciled with (J) after we size up records.</p>
<h4>Sizing up records or aggregates</h4>
<p>Records are aggregates of data that form a cohesive whole. The individual bits of data - the attributes of the whole - are typically of different types. The size of such a datum can only be expressed as an enumeration of the different kinds of data units.</p>
<pre><code>size(record) = sum(size(attributes)) expressed as n1 T1 units, n2 T2 units and so forth
               where there are n1 number of units of type T1 and so forth
                                                                                     --(K)</code></pre>
<h5>Extrapolation: List as a record</h5>
<p>Now we can go back and reconcile (J2) with (J):</p>
<pre><code>Using (J2), size(list) = size(record(list attrs, list items))
Using (K),             = size(list attrs) and size(list items)
Using (J1),            = size(list attrs) and N item type-list units</code></pre>
<h5>Extrapolation: Program as a record</h5>
<p>On a grand-enough scale, a program can be considered a record of code, data and metadata. Thus,</p>
<pre><code>size(program) = size(code) + size(data) + size(metadata)
              = C turings  + D data units + M metadata units                         --(L)</code></pre>
<p>Of course, <code>program</code> could be replaced by <code>application</code>, <code>system</code> or <code>set of applications</code> with the same result. Note that this is the &quot;Stored Program Size&quot; that we started off this digression to discover.</p>
<p>There are still some pieces to tie down, however, so lets proceed on to other common compound data types.</p>
<h4>Sizing up Tables</h4>
<p>Tabular data in the natural world are used when items in a list have multiple attributes and comparing/contrasting them together is useful. For example, a table comparing features of two brands of a fridge is useful when in the market for one. The defining characteristics of a table are:</p>
<ul>
<li>Each row represents one &quot;thing&quot; in the domain, naturally represented as a record of its attributes.</li>
<li>Each column represents the attributes of interest for each thing.</li>
</ul>
<p>The second major use of a table is to lookup information based on attributes. A table of school grades could be browsed to find a particular student&#39;s grade if we know his/her name or student ID, for example.</p>
<p>The third major use of tables is to derive new or summary information - combine the information from multiple source tables to form a new one.</p>
<p>These uses of real-world tables are represented in multiple implementation data structures - multi-dimensional arrays, Hashmaps or associate arrays and relational database tables (which are themselves a collection of some complex implementation data structures like BTrees) to give a few examples.</p>
<p>The choice of the actual data structure used in the implementation is usually dependent on how the data is used. For example, the table with student grades is most likely to be queried to find a particular student&#39;s grade, so access to the grade given a student ID is optimized most and that governs how the data in the logical table is stored. Duplication, artificial references and links are all valid additions to the implementation data structure to optimize the access path.</p>
<p>For purposes of size, however, how we access the data is immaterial. The data (as understood in its domain, not the implementations&#39;) is still rows of records. Therefore we can say with some surety that:</p>
<pre><code>size(table) = sum(size(record))                                                      --(L)</code></pre>
<h4>Sizing up Hierarchical Data</h4>
<p>Hierarchical data is any collection of data where items are related to each other with a strict parent-child link. Family trees and Cabinet+Folder+files are typical examples. The size of such data can be readily computed as:</p>
<pre><code>size(hierarchy) = size(root) + size(root&#39;s children)
size(root)      = size(compound)
size(children)  = size(list(child items))
                                                                                     --(M)</code></pre>
<p>Hierarchical data is usually implemented as linked lists (in imperative, per-object environments) or foreign keys (in declarative, per-collection environments). The same points of applying &quot;hierarchy-level attributes&quot; mentioned above for lists apply to hierarchies as well.</p>
<h4>Sizing up Graphs</h4>
<p>Graphs focus on items/objects/entities and the relationships between them. A map of all possible flights in a country is a graph, as is a network of relationships between friends. The key concepts in a graph, therefore, are nodes and edges. The size of a graph can readily be calculated as:</p>
<pre><code>size(graph) = sum(size(node)) + sum(size(edge))
size(node)  = size(compound)
size(edge)  = size(reference to end node)
                                                                                     --(N)</code></pre>
<p>Graphs are typically implemented in one of three ways:</p>
<ul>
<li>As linked lists, when traversal is the primary application</li>
<li>As adjacency lists</li>
<li>As nxn matrices</li>
</ul>
<p>These implementations have their own size characteristics, obviously.
TODO: FIGURE OUT IF THERE ARE IMPLICATIONS OF THIS SO A BETTER MEASURE IS ADVISED.</p>
<h4>Compound items contained within other compound data</h4>
<p>TODO: REWRITE THIS AFTER FIXING OTHER SECTIONS TO MAKE THEM 2D</p>
<p>What if the items in the list are themselves compound? In all likelihood, a list of students will have more than one attribute of the student; so the student list item will actually be a record of all such interesting attributes. Using (J):</p>
<pre><code>size(student list) = 0 + sum(size(student record))</code></pre>
<p>Assuming we have a first name, last name and grade as the record attributes,</p>
<pre><code>size(student record) = size(first name) + size(last name) + size(grade)
                     =  1 name unit     + 1 name unit     + 1 real number unit
                     =  2 name units + 1 real number unit.</code></pre>
<p>Thus:</p>
<pre><code>size(student list) = sum(size(student record))
                   = size(student record) x number of students in list
                   = [ 2 name units + 1 real number unit] x N,  where N is the number of students in the list</code></pre>
<h3>Dynamic Size</h3>
<p>Like code, data too can vary over time - program execution changes sizes of data structures as do real-world events. At a particular aggregate level, this could be seen as the change in size of that aggregate - its dynamic size.</p>
<p>For most uses of this theory, we&#39;d be interested in the maximum possible dynamic size that some data could achieve. That is,</p>
<pre><code>dyn_size(data) = max(size(data)) across all instants of time
                                                                                     --(O)</code></pre>
<p>Note: There is a difference between static and dynamic data and static and dynamic <em>sizes</em> of data. A program will have both static and dynamic data: the former being data allocated at startup and the latter being data allocated during execution. Both these types of data, however, will  dynamic sizes - their sizes can change over time. For example, an array might be allocated a size of 100 at startup, but only 5 are used with the rest expected to be filled during execution.</p>
<p>(O) is an interesting equation because this is exactly the definition of Big Omega. That theory, of course, takes the next logical step and says that the maximum size is proportional to some attribute of the data, for eg, the number of items in it, usually referred to as <code>N</code>. Thus:</p>
<pre><code>max(size(data)) is proportional to N
                                                                                     --(P)</code></pre>
<p>This means all of the theory surrounding algorithms can be readily used to come up with dynamic size of data contained in programs, only with the unit being the new (unnamed) data units.</p>
<h3>Code as Data and vice versa</h3>
<p>Part of the reason we embarked on this detour of data size is to size up code when it has data-like properties; especially when its stored in a passive form. So, how would we size up code as data? Well, in its most general depiction, code can be considered a graph. Thus:</p>
<pre><code>size(code) = size(graph of instructions)
                                                                                     --(Q)</code></pre>
<p>This does not, of course, account for dynamic code itself, so an equation similar to (O) could be written for dynamic size of code:</p>
<pre><code>dyn_size(code) = max(size(code)) across all instants of time
                                                                                     --(R)</code></pre>
<p>TODO: DATA AS CODE: READ SICP AGAIN AND SEE IF THERE&#39;S A HAPPY CONVERGENCE OF IDEAS. THIS MIGHT ALSO BRING THE TURING AND CHURCH VERSIONS TOGETHER WITH THIS THEORY.</p>
<h3>Name for the data unit</h3>
<p>Options for name: Chris Date or Edgar Frank &quot;Ted&quot; Codd. since date is also a datatype, maybe not Date; and Codd seems a little odd. I&#39;m leaning towards Ted.
STOPPED HERE JAN 13 AM</p>


</body>
</html>
