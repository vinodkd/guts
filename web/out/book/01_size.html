<html>
<head>
  <title></title>
  <link rel="stylesheet" type="text/css" href="css/guts.css"></link>
</head>
<body>

<h1>Code Size</h1>
<ul>
<li><a href="outline">Outline</a></li>
<li><a href="#what_is_size">What is size?</a></li>
<li><a href="#struct_of_code">The basic structure of code</a></li>
<li><a href="#sizing_up_hw">Sizing up Hello World</a></li>
<li><a href="#ssi">Down to the basics - SSI</a></li>
<li><a href="#turtles">Turtles all the way</a></li>
<li><a href="#back_up">Back up the rabbit hole</a></li>
<li><a href="#turing_unit">A new measure of code size</a></li>
<li><a href="#sizing_data">Sizing up the data</a></li>
<li><a href="#static_v_dynamic">Static vs Dynamic size</a></li>
<li><a href="#engg_corner">Engineer&#39;s Corner: Implementing Turing sizes</a></li>
</ul>
<p><a name="outline"></a></p>
<h2>Outline</h2>
<p>why count size, compare with sloc and other current ways of counting size, basic structure of code: language, program, operation, sizing up Hello World as an example, therefore compound operations and containers, still not found size of operations, therefore SSI, add viz as Lego blocks, size of sequence, size of if, add viz as pipes, size of loop, add viz of marble run, <strong>add viz about graphs</strong>, still not found size of operations other than if and loop, therefore go down one level, then infinite levels, therefore resolve that at asymptote of the turing machine all sizes are 1, therefore define that all we need is a relative &quot;base&quot;, then back up to hello world picking the java language as the base and count size that way, then present comparison with sloc again, then introduce the &quot;turing&quot; as the size unit relative to any base, then answer questions about sizing 2 langs relative to a base, <strong>then talk about platform affordances</strong>, <strong>then talk about sizing up static data, then talk about static vs dynamic data, then move to engineering and the simple and exact ways of counting size</strong>.</p>
<p><a name="what_is_size"></a></p>
<h2>What is size?</h2>
<p>The common sense description of size is: </p>
<pre><code>    Size: How big something is. </code></pre>
<p>Length, area and volume are typical ways of quantifying how big something is. When applied to code, this essentially means how much code there is; specifically excluding ideas like how complex it is, how fit it is for its intended purpose and so forth.</p>
<p>The most common real-world use of such a measure is estimation: when building new software or changing existing ones we like to estimate the effort involved. Is there a larger purpose, however - one that serves the practitioner and not just the manager?   </p>
<p>I posit that size is one of those basic properties of code that aids in <em>understanding</em> and <em>knowing</em> code as well. We talk routinely of small, well designed codebases that can be understood and used easily; and also of large, unwieldy codebases that are difficult to understand. Wouldn&#39;t it be nice to quantify those subjective statements? A case might therefore be made that size is one of the &quot;thing&quot;s that we <em>should</em> understand about code. Also, note that the &quot;small&quot; and &quot;large&quot; parts mentioned above point to the size of the code and  the &quot;easy&quot; and &quot;difficult&quot; parts point to its complexity; and certainly the two are related. So quantifying size might lead to understanding other &quot;thing&quot;s about code.</p>
<h3>Current notions of code size</h3>
<p>Let&#39;s first look at how size is measured currently. In my cursory review of the current state of affairs, there are two broad approaches to measuring size:</p>
<ol>
<li><strong>Empirical</strong>: The aim here is to have numbers that represent size so that decisions and actions can be taken based on them. <a href="http://tbd/">Source Lines of Code</a>, <a href="http://tbd/">Function Points</a>, <a href="http://tbd/">COCOMO</a>, <a href="http://tbd/">Structure 101&#39;s fatness metric</a> and <a href="http://tbd/">Halstead metrics</a> seem to fit into this category.</li>
<li><strong>Descriptive</strong>: These measures of size do use numbers that are easily derived from the source code, but they are used primarily to visualize it so that humans can grok it and take appropriately intelligent action. Code visualization tools such as <a href="http://tbd/">Code city</a>, <a href="http://tbd/">Software cartography</a> and Alan Kay&#39;s <a href="http://tbd/">Empire state building made of A4 sheets of code</a> are some examples of this type of measure.</li>
</ol>
<p>Here is a comparison of the pros and cons of these two approaches:</p>
<table>
    <thead>
        <tr><th>Approach</th><th>Pros</th><th>Cons</th></tr>
    </thead>
    <tbody>
        <tr>
            <td>Empirical</td>
            <td>
                <ul>
                    <li> Actually measurable (with some error, but still) </li>
                    <li> Language agnostic</li>
                    <li> Useful as gross comparison tools.</li>
                    <li> Relatively easy to apply to small programs manually; and gracefully upgradable to bigger codebases with automation.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Not &quot;smart&quot; units of measure in that they don&#39;t consider the structure of code <em>AS CODE</em>. With the possible exception of the Halstead metrics, the other approaches treat code as either text or conceptual &quot;functions&quot;.</li>
                    <li>Not accurate measures of size per se; just convenient ones.</li>
                    <li>Not easily extended to other properties of code; the relations are forced and the equations (if any) have empirical constants and pre-conditions attached.</li>
                </ul>
            </td>
        </tr>
            <td>Descriptive</td>
            <td>
                <ul>
                    <li> Easy for humans to understand a lot of information in one go. </li>
                    <li> Able to transcend the implementation language depending on the abstraction chosen.</li>
                </ul>
            </td>
            <td>
                <ul>
                    <li>Don&#39;t give direct feedback as to what&#39;s wrong; a practiced user has to interpret the results.</li>
                    <li>Do not actually measure anything at all; merely provide a visualization.</li>
                </ul>
            </td>
        </tr>
    </tbody>
</table>

<p>Obviously, neither approach seems comprehensive nor feels right. Wouldn&#39;t it be nice to have a measure of code size that:</p>
<ul>
<li>Used the structure of the code <strong>AS CODE</strong>,</li>
<li>Was language agnostic,</li>
<li>Was indeed measurable,</li>
<li>Was extensible to define other properties of code with,</li>
<li>And yet was easy for humans to understand and use to understand a lot of information in one go?</li>
</ul>
<p>That&#39;s what I&#39;d call a natural measure of software size. The rest of this chapter is an attempt to build one. My approach is part engineering, part science: I&#39;d like to develop a theory that is general enough, but with sufficient focus on how to apply it to real world software.</p>
<p><a name="struct_of_code"></a></p>
<h2>The basic structure of code</h2>
<p>Programs, in general, can be described as a set of instructions or operations to do a certain task. The set of instructions or operations that make up a particular program is a subset of the available instructions or operations, and this superset is usually called a language. I&#39;ll repeat these concepts as definitions so that they can be referred to later:</p>
<pre><code>    Language: A finite set of instructions or operations that can be used to write code.
              All code is written in languages.
    Operation: The smallest unit of independent code execution in a language.
           It is the basis of size measurement because of its atomicity.
                                                                                     --(1)</code></pre>
<p>Or in BNF-ish syntax:</p>
<pre><code>    language  :=  operation+
    program   :=  operation+ where operation belongs to language
    operation :=  ....smallest unit of code exec....
                                                                                     --(2)</code></pre>
<p>... which only highlights the fact that while we have a definition for Operation, we&#39;re not really close to using it to actually measure things. However, <strong>if</strong> we knew how to size operations, this characterization seems to intuitively say that the size of a program is a summation of the size of the operations contained in it. In hand wavy formulas, this can be expressed as:</p>
<pre><code>    size(program)      = sum(size(operation)) for all operations in the code
    assuming
    size(operation)    = a known value                                               --(3)</code></pre>
<p>This would be a nice result, but it still remains to be determined if size can indeed be characterized this way. This chapter will attempt to do just that.</p>
<p><a name="sizing_up_hw"></a></p>
<h2>Sizing up Hello World</h2>
<p>Let&#39;s start by trying to size up the most common program of all - the Hello World program - and use that to refine our intuitive expectation of summing up sizes. For specificity, I&#39;ll use Java, but I&#39;m using Java as an example of programming languages in general, not as <em>the</em> defining language for this theory. Here&#39;s Hello World in Java:</p>
<pre><code>    // program 1
    public class HelloWorld{
        public static void main(String[] args){
            System.out.println(&quot;Hello World&quot;);    //line 1
        }
    }
    // SLOC: 5, Size: ?</code></pre>
<p>So this is a program that weighs in at 5 SLOCs; but if we were to consider it as code instead of lines of text, what would its size be? </p>
<p>Now, if you&#39;re not a Java person, you&#39;re likely to complain that I chose a bad language to start with; and I&#39;d almost agree. There&#39;s actually only one line of code in there - the one that prints the message; everything else is ceremonial structure. However, it does help expose the fact that code written in any language eventually has some superstructure; and in that sense Java&#39;s requirement to expose such structure is more useful for our size-measuring purposes than other languages&#39; &quot;hiding&quot; of such structure &quot;under the carpet&quot;, so to speak.</p>
<p>So, warts notwithstanding, let&#39;s see if we can use this example to examine the idea of code size. From line 1 alone, quite a few questions arise:</p>
<ol>
<li>Line 1 actually has 1 package access, 1 object access and a function call - not very atomic. Is it a single Operation, really?  </li>
<li>What about the function call itself? <code>println()</code> is it&#39;s not an actual language feature; it&#39;s a method in a library. Should we actually count its call as an Operation?</li>
<li>... And while we&#39;re counting, should we count the size of the method itself when all we&#39;re doing is calling it?</li>
</ol>
<p>If we factor in the rest of the program, more questions arise:</p>
<ol>
<li>Is the class definition an Operation? Similarly, what about <code>main()</code>&#39;s definition?</li>
<li>SLOC is a clearly understandable number. How do we put numbers against this size concept?</li>
</ol>
<p>Let&#39;s try to answer each question and form some opinions along the way.</p>
<h3>Question 1: Is System.out.println(...) one Operation?</h3>
<p>Let&#39;s contrast Program 1 with some similar code that might flesh Qn #1 out better:
Here&#39;s one contrasting program:</p>
<pre><code>    // program 2
    import java.io.PrintStream;
    public class HelloWorld{
        public static void main(String[] args){
            PrintStream outPS = System.out;    // line 1.1
            outPS.println(&quot;Hello World&quot;);      // line 1.2
        }
    }
    // SLOC: 7, Size &gt; Size(Program 1)?</code></pre>
<blockquote>
<p>Aside: Note that Program 2&#39;s SLOC went up because <code>PrintStream</code> had to be imported in, while Program 1 doesn&#39;t need that line because of &quot;platform&quot; affordances. More on such platform implications later.</p>
</blockquote>
<p>Now, program 2 is admittedly contrived, but it&#39;s reflective of similar contrasts in real code where the latter representation would be useful [<a href="#ftnote1">1</a>]. It splits out the original line 1 into two, separating the individual steps involved and it does increase the size of the program even if with an &quot;unnecessary&quot; addition of a local variable; but it highlights the fact that <code>System.out.println(...)</code> is something more than atomic - it&#39;s a <strong>Compound Operation</strong>, if you will.</p>
<p>[1]: For e.g. where you&#39;d split a long chain of method calls into an intermediary value for readability, but this increases size because now you have an additional variable</p>
<p>In fact, we could take it a step further and do this:</p>
<pre><code>    // snippet 2.1
    PrintStream outPS;                 // line 1.1.1
    outPS = System.out;                // line 1.1.2
    outPS.println(&quot;Hello World&quot;);      // line 1.2
    // SLOC : 3, Size : ?</code></pre>
<p><em>Now</em> it cannot be broken down anymore and therefore matches our definition of a <strong>Operation</strong> from above. So if we took the definition to heart and broke line 1 down this way, 3 &quot;smallest possible&quot; Java statements are required to make it. This means we have to recognize that some operations can be aggregates and add that as a definition:</p>
<pre><code>    Compound Operation: An operation that can be broken down into smaller operations within the Language&#39;s set of operations.
                                                                                     --(4)</code></pre>
<p>... and update the BNF-ish version (2) to include this fact:</p>
<pre><code>    language  :=  operation+
    program   :=  operation+
    operation := compound_op | simple_op
    simple_op :=  ....smallest unit of code exec....                                 --(5)</code></pre>
<p>...and update formulas (3) to reflect this:</p>
<pre><code>    size(program)            = sum(size(compound_op |simple_op)) for all operations in the code
    size(compound_op)        = sum(size(simple_op)) for all simple_ops that can replace the compound op.
    assuming
    size(simple_op)          = a known value                                         --(6)</code></pre>
<h3>One more variation of Program 1</h3>
<p>Are we done with Qn #1? Not quite; here&#39;s another version of the same program:</p>
<pre><code>    // program 3
    public class HelloWorld{
        public static void main(String[] args){
            System.out.print(&quot;Hello &quot;);
            System.out.println(&quot;World&quot;);
        }
    }
    // SLOC: 6, Size = Size(Program 2)?</code></pre>
<p>This program splits the final function call itself into two while still achieving the same end result. Clearly an operation can not only be broken into smaller bits, but what it <em>does</em> can also be broken into smaller bits. This hints at a couple of things:</p>
<ul>
<li>That a given problem can be solved by multiple programs. It seems intuitive that there&#39;s at least one program that solves it with the least size and that there are potentially more than one programs that solves it with more.</li>
<li>What a program IS (structure and its size) does not predict what the program DOES (its meaning). We will have to look elsewhere for meaning - either downwards to the platform that it is based on or upwards to the human (or proxy human i.e., code generator) that creates it. </li>
</ul>
<h3>Question 2: Is calling a library function an Operation?</h3>
<p>While a library function itself is not a language feature, the ability to make a function call certainly is. A function call is essentially a shortcut to invoke a bunch of statements defined elsewhere; and supporting such a feature does require some effort on the language runtime&#39;s part. So Operation it is. In definition form again:</p>
<pre><code>Defining and calling functions are Operations if the language supports it            --(7)</code></pre>
<h3>Question 3: Should we count the size of the function if all we&#39;re doing is calling it?</h3>
<p>The size of the function itself (i.e., that of its body) should certainly be different from that of a call to the function. This answer highlights the fact that these two constructs - the definition of a function and the call to it - are two separate operations that a language supports. The definition is clearly a compound operation, but what about the call? If the language supports the concept of functions and calling them, the operation of calling a function has to be atomic, by definition. Updating the BNF-ish definitions, therefore:</p>
<pre><code>    operation   := compound_op | simple_op
    compound_op := function_def | ...other compound ops...
    simple_op   := function_call | ....other smallest units of code exec....         --(8)</code></pre>
<p>Onto Question #4.</p>
<h3>Question 4: Is a class an Operation? What about functions?</h3>
<p>At first sight, this question seems trivially answerable: line 1 in Program 1 is the only &quot;working&quot; Operation, so the others shouldn&#39;t have much importance. However, it becomes interesting when contrasted with other languages that do not necessarily need containers such as classes or a predetermined function like <code>main</code>. The same hello world in ruby (or Python), for example, would be:</p>
<pre><code>    # program 4
    print &quot;Hello World&quot;, &quot;\n&quot;    #SLOC:1, Size: ?</code></pre>
<blockquote>
<p>Note: Yes, I know that this is possible because of &quot;Platform affordances&quot; and that behind the scenes are some intelligent defaults. I also realize that this is not the same as Java&#39;s <code>System.out</code>. As mentioned before, we&#39;ll deal with this &quot;tip of the iceberg&quot;-ness later.</p>
</blockquote>
<p>Now it seems intuitive that the first program is &quot;larger&quot; than the second one, doesn&#39;t it? Or, to contrast in the other direction, what if we wrote our HelloWorld.java like so?</p>
<pre><code>    // program 5
    public class HelloWorld{
        public static void main(String[] args){
            greet();                //line 1
        }
        public static void greet(){
            System.out.println(&quot;Hello World&quot;);    //line 2
        }
    }
    // SLOC: 8, Size: ?</code></pre>
<p>This (contrived) version of the code is obviously bigger than the previous - while doing the same thing as before.</p>
<p>So Q#4 really is: Does a language&#39;s container structures contribute to its size? Going by the examples above where they &quot;add to the structure&quot;, I would argue that they do; and therefore should be considered &quot;compound&quot; <strong>Operations</strong> s with some non-zero size of their own. This also answers the question about <code>main()</code>&#39; - and by extension - all functions: functions are containers and therefore have a non-zero size of their own in addition to contributing whatever size their contents have to the total size.</p>
<p>Updating  definitions from (7) and (8) above to include these facts, therefore:</p>
<pre><code>    compound_op       := class_def | function_def | ...other compound ops...
    class_def         := class_construct + class_contents
    class_contents    := method_def*, field*
    method_def        := function_def
    field             := operation
    function_def      := function_construct + function_contents
    function_contents := operation+                                                  --(9)</code></pre>
<p>and</p>
<pre><code>    size(compound_op)        = size(function_def) | sum(size(simple_op)) for other compound ops.
    size(class_def)          = size(class_construct) + size(class_contents)
    size(class_contents)     = size(method) + size(field) for all methods and fields in the class
    size(method)             = size(function_def)
    size(field)              = size(operation)
    size(function_def)       = size(function_construct) + size(function_contents)
    size(function_contents)  = sum(size(operation)) for all operations in the function
                                                                                     --(10)</code></pre>
<p>Or more generally,</p>
<pre><code>    size(container)         = size(container_construct) + size(contents)
    size(contents)          = sum(size(operation)) for all operations in container
                                                                                     --(11)</code></pre>
<p>Using (11) we can easily extrapolate to larger structures:</p>
<pre><code>    size(app)             = size(app_construct)             + sum(size(programs in app))
    size(program)         = size(program_construct)         + sum(size(package|modules|classes|functions in program))
    size(package|module)  = size(package|module construct)  + sum(size(classes|functions in module|package))
                                                                                     --(12)</code></pre>
<p>Looking at (11) above, however, how different is a container from another Atomic Operation in terms of contribution to size? Its presence adds to the total size just as another atomic operation and the formula above ensures that its contents&#39; size is accounted for. As long as we retain the <code>+ sum(size(contents))</code> part, we could treat containers as some more atomic operations during analysis.</p>
<h3>Question #5: So how to numerically calculate size?</h3>
<p>So how do we put some numbers against these ideas? We&#39;re slightly better off than before, but we still don&#39;t yet know to size the atomic operations, nor do we know the size of the containers themselves. All we have determined so far is: <em>If</em> we know the sizes of the operations and their containers, the overall size is an accumulation of individual sizes.</p>
<h3>Review</h3>
<p>Ok, so <code>Hello World</code> helped us understand compound operations and containers, but didnt help much with understanding the nature and size of the operations. Maybe a program in a high level language is closer to the application of programming as opposed to its core. </p>
<p>Maybe we need to look at a simpler model of programming?</p>
<p><a name="ssi"></a></p>
<h2>Down to the basics - SSI</h2>
<p>Let&#39;s try the simplest notion of code. All of programming has <a href="http://en.wikipedia.org/wiki/Structured_programming#Low-level_structure_programming">famously</a> <a href="http://en.wikipedia.org/wiki/Structured_program_theorem">been</a> <a href="http://en.wikipedia.org/wiki/Von_Neumann_architecture">depicted</a> as being made of 3 basic operations:</p>
<ul>
<li>Sequence, i.e. steps done in sequence</li>
<li>Selection, i.e. the choice of one step vs the other, aka the <code>IF</code></li>
<li>Iteration, i.e. the ability to repeat steps that have been already been executed, aka the <code>LOOP</code></li>
</ul>
<p>Let&#39;s try sizing up each of these basic operations, starting with...</p>
<h3>Sequence</h3>
<p>A Sequence is technically not an operation itself, but a string of them. Let&#39;s plod on, however, to see if we can glean something about operations from a collection of them. Let&#39;s write a simple <em>Sequential</em> program like &quot;Print 5 superhero names&quot;. I&#39;m going to switch to pseudo-code so that we can ignore all platform issues. Here&#39;s the program in a language that somewhat resembles assembly:</p>
<pre><code>    // program 6
    print &quot;Superman&quot;
    print &quot;Batman&quot;
    print &quot;Green Lantern&quot;
    print &quot;Green Arrow&quot;
    print &quot;Aquaman&quot;
    stop
    // SLOC: 6, Size: 6* units</code></pre>
<p>Since we&#39;re at the same impasse of not knowing the sizes of operations, let&#39;s make some assumptions. Assuming the size of the <code>print</code> and <code>stop</code> operations are 1 unit, and using (3):</p>
<pre><code>    size(program6) = sum(size(5 print operations &amp; 1 stop operation))
                   = 1* + 1* + 1* + 1* + 1* + 1*
                   = 6* units
    (the * is to remind us that sizes being 1 is an assumption)                      --(13)</code></pre>
<p>... is 6* units. This is sort of similar to counting lines of code and fits our common sense notion that the code is &quot;6 units long&quot; or &quot;6 units tall&quot;.</p>
<p>Would that be long or tall? Here&#39;s where a little physical analogy might help. </p>
<p><img src="images/lego-tower.jpg" alt="Kid builds a Lego tower" title="This is your Code">
<img src="images/lego-tower-falling.jpg" alt="Kids&#39; Lego tower falls" title="This is your Code crashing"></p>
<p>We talk all the time about &quot;building&quot; software and code building has long been equated to piling Lego blocks on top of each other. And when heavy code breaks it crashes very similarly to how a heavy Lego structure does. So let&#39;s equate size to height. </p>
<p>One break from the normal way of using Legos, though: code Legos are indeed stacked one block at a time, but by sticking each block <strong>under</strong> the one that&#39;s already in place, not on top. That way, program 6 gets built in the order we read it, not from the last statement upwards.</p>
<p>But that was just a <em>Sequence</em>. Let&#39;s try some ...</p>
<h3>Selection</h3>
<p>... by writing a simple program to check if 5 is odd or even. Again, in pseudo code:</p>
<pre><code>    // program 7
    rem = 5 % 2
    if rem == 1 then
        print &quot;5 is odd&quot;
    else
        print &quot;5 is even&quot;
    endif
    stop
    // SLOC: 7, Size: 5* sq. units</code></pre>
<p>This program is not just tall, it is wide too. Until the <code>if</code> is encountered, things are linear, but at that point we could go one of two ways. This can be visualized as a &quot;left+right&quot; pair or a &quot;down+side&quot; pair, something like so:</p>
<pre><code>    // program 7 (alt view)
    +----------------+
    |rem = 5 % 2     |
    +----------------+-------------------+
    |rem == 1 ?      |                   |
    |    true        |    false          |
    +----------------+-------------------+
    |print &quot;5 is odd&quot;| print &quot;5 is even&quot; |
    +----------------+-------------------+
    |stop            |
    +----------------+</code></pre>
<p>So, what then, is the size of an <code>if</code>? The size of the main branch contributes to the length of the program it&#39;s in, while the size of the alternate branch can be considered as adding to the width of the program. It seems safe to say, therefore, that:</p>
<pre><code>    size(if)      = size(condition check) + sum(size of individual branches)
                  =  c + sum(b) 
    where
                c = a non-zero size of the condition check
                b = size(branch)  
                  = 1 (width) x h (height of branch)                                 --(14)</code></pre>
<p>Applying this to the <code>if</code> in program 7 and assuming the condition check is a size 1 (because there&#39;s only one comparison being done), we get:</p>
<pre><code>    size(program7&#39;s if) = size(condition check) + size(if branch) + size(else branch)
                        = 1* + 1 x 1* + 1 x 1*
                        = 3* sq. units                                                --(15)</code></pre>
<p>To use this result in calculating Program 7&#39;s size, we&#39;ve to represent all sizes as &quot;areas&quot; first. Assuming again that the <code>print</code> operation was 1 unit tall, we should add that it is 1 unit wide. Program 7&#39;s size(area) therefore becomes:</p>
<pre><code>    size(program 7) = sum(size(operations))
                    = size(assignment operation) + size(if) + size(stop)
                    - size(modulus op + assignment) + size(if) + size(stop)
    Now assuming size(modulus op) = 1, we get
                    = 2* + 3* + 1*   assuming the assignment operation is also of size 1.
                    = 6* sq. units, compared to a SLOC of 7.                          --(16)</code></pre>
<p>For completeness, let&#39;s convert Program 1&#39;s size to &quot;area&quot; units as well:</p>
<pre><code>    size(program 6) = 6* sq. units                                                    --(17)</code></pre>
<p>Note that program 7 has a SLOC of 7, but a size of 6* sq. units. Smaller numerically, but larger by size and semantics.</p>
<p>Does our Lego block analogy still hold up, though? The <code>if</code> requires data to be used, a decision to be made and one of (potentially many) alternate routes to be taken. This is probably best visualized as something &quot;flowing&quot; from one statement (i.e. block) to another, with control points to direct flow. Maybe pipes are a better analogy therefore?</p>
<p><img src="images/pipes.jpg" alt="Code as pipes" title="This is your Code&#39;s plumbing">
<img src="images/pipes2.jpg" alt="Code as pipes" title="This is your Code&#39;s plumbing"></p>
<p>Sorry I couldn&#39;t find better pictures, but hope these convey some part of the idea. Every time you see a T-junction or a Cross, think <code>if</code> operation; otherwise the flow of liquid represents the Sequence.</p>
<p>Unsatisfying as the pictures are, more so is the analogy. While data does flow from one operation to another, it doesn&#39;t flow like a liquid does. Data in a digital computer is discrete and better described as chunked than fluid. Is there a better physical analog? What we need is something that is built using standard parts (like Legos) and allows things to flow through them (like pipes) but only allows solid things. </p>
<p>Without further ado, I present:</p>
<p><img src="images/marble-run.jpg" alt="Code as a marble run, data as marbles" title="Code = pipes, data = marbles">
<img src="images/marble-run-pieces.jpg" alt="Marble run pieces" title="Note that one piece - the purple one - is a simple logic gate "></p>
<p>... the marble run! It does everything we would like our physical analog of code to do and then some. It has the standard blocks that link together obviously (which is a slight difference - that the connectors are fixed to the blocks); but it also has &quot;source&quot; and &quot;sink&quot; pieces, pieces that change direction (not all of which are logically important) and even pieces that have some built-in logic. If you look closely you&#39;ll find that one of the purple pieces is a simple flip-flop (aka <code>IF</code>)- it sends successive marbles down alternate paths.</p>
<p>So it seems that the marble run is indeed a good choice as our physical analog for code. We will use it only as a mental model in our theory forming activity, but there <strong>are</strong> <a href="http://www.hackerspace.lu/2012/01/21/marble-adder/">real world marble runs that have been created to do actual computations</a>; so it&#39;s certainly an apt choice.</p>
<p>Ok, enough fun. Let&#39;s try the final operation ...</p>
<h3>Iteration</h3>
<p>... by writing a simple loop to print 1 to 5. Again in pseudo code:</p>
<pre><code>    // program 8
    loop i = 1 to 5
      print i
    end loop
    stop
    // SLOC : 4, Size: ?</code></pre>
<p>Written in this form, it seems like the <code>loop</code> is a short cut to write out a long sequence of operations. Indeed, program 8 can be rewritten as:</p>
<pre><code>    // program 8A
    print 1
    print 2
    print 3
    print 4
    print 5
    stop
    // SLOC : 6, Size: 6* sq. units</code></pre>
<p>Such &quot;unfolding&quot; of loops is not uncommon; and viewed this way we could conclude that a loop&#39;s primary size is its height, which is equal to the number of operations within the loop times the number of times those operations are looped around. Not all loops can be unfolded thus, however, as a simple example that uses a <code>do-while</code> loop or an infinite loop will attest. However, there&#39;s an alternate way to express a loop, presented below. This will work for any kind of loop including ones whose number of iterations cannot be determined up-front. </p>
<pre><code>    // program 8B
          i=1
     top: if not(i &lt;= 5)      // line 1
               goto end 
          else
              print i
              i = i + 1
              goto top        // line 2
          end if
    end: stop
    // SLOC : 9, Size : 7* sq. units</code></pre>
<p>Now the true nature of <em>Iteration</em> becomes obvious: <code>Iteration = if + goto</code>. The <code>if</code> sets up the conditions for iteration and the <code>goto</code> executes it. The <code>goto</code> is therefore the key ingredient in getting iteration to work, so let&#39;s try to understanding it a little better.</p>
<p>A goto is a route from one &quot;block&quot; to another, a connector. Program 8B has 2 obvious kinds of gotos and one that&#39;s not that obvious:</p>
<ol>
<li>The conditional goto that is guarded by a comparison (line 1);</li>
<li>The unconditional goto that just jumps to another location (line 2); and</li>
<li>The implicit goto between operations.</li>
</ol>
<p>The <em>Unconditional <code>goto</code></em> alters the flow of execution and skips ahead to another location, adding the &quot;width&quot; (or more generally, another dimension) to the program. If the destination of the <code>goto</code> is not local to the point of branch off, the impact on size is somewhat difficult to determine.</p>
<p><em>Conditional <code>goto</code>s</em> remove some of that uncertainty by checking a condition before branching. Adding the check reduces the chance of an invalid target or that of &quot;spaghetti code&quot; - an unholy tangle of wild gotos that only makes sense when you write it.  </p>
<p>However, conditional gotos only reduce the chance of indeterminate size, they do not eliminate it. For e.g., the <code>goto end</code> line in program 8B could lead to some location that is far away from the rest of the code. So the deciding factor for size of a goto is whether or not its destination is known. Let&#39;s call the ones with known, defined destinations like &#39;top of the loop&#39; as _Bound Goto_s and the ones that are not such as _Unbound Goto_s. Their sizes, therefore, are:</p>
<pre><code>    size(bound goto)  = 1* width x N height already counted elsewhere
                      = 1* width x 1* height
                      = 1* sq. units                                                  --(18)</code></pre>
<p>That is, the height of a bound goto exists, but it has typically been already considered as part a &quot;larger structure&quot;, so only the contribution of the single goto &quot;block&quot; need be considered.</p>
<pre><code>    size(unbound goto) = G
    where            G = 1 unit height x G1 width  or
                       = 1 unit width x G2 height                                    --(19)</code></pre>
<p>That is, while we could measure one or more &quot;dimensions&quot; of the unconditional goto, there will always be one dimension that we cannot quantify and therefore its overall size remains an unknown quantity.</p>
<p>Finally, <em>Implicit <code>goto</code>s</em>: On the &quot;main line&quot; of code, implicit gotos guide the execution of code by stringing successive operations together. In fact the machine executing these programs (or any general computer, for that matter)  can be thought of executing this meta-program:</p>
<pre><code>    // meta program 1
    1: read a specific location for the address of the next instruction to execute
    2: execute it
    3: if step 2 didn&#39;t set the next instruction to execute, auto increment to next address in the same location
    4: goto 1</code></pre>
<p>So the gotos exist, even if we do not depict them in code at the level of normal discourse. The difference between these gotos and the others is that they connect one operation to another &quot;by default&quot; i.e., in the most obvious way that they are supposed to be connected. As such, it&#39;s safe to posit that they do not contribute to the size. That is,</p>
<pre><code>    size(implicit goto) = 0                                                          --(20)</code></pre>
<p>In program 8B, both the <code>goto</code>s are well-behaved. They don&#39;t fly off to kingdoms unknown: they go to the top of the loop or exit it - two very well-known spots. So they are clearly bound gotos. So the size of program 3B would be:</p>
<pre><code>    size(program 8B)     = sum(size(operations))
                        = size(assignment) + size(if) + size(stop)
                        = 1* + size(condition) + size(branches) + 1*
                        = 1* + 1* + (1 x 1* + 3 x 1*) + 1*
                        = 2* + 4* + 1*
                        = 7* sq. units                                                   </code></pre>
<p>More generally, the loop in program 8B could be written in template form as:</p>
<pre><code>    // program 8B-templatized
          &lt;&lt;.. steps before loop..&gt;&gt;
          &lt;&lt;initialize loop&gt;&gt;
     top: if not &lt;&lt;loop condition&gt;&gt;
           then
               goto end
           else
            &lt;&lt;loop body&gt;&gt;
            &lt;&lt;increment loop&gt;&gt;
            goto top
          end if
    end:  &lt;&lt;.. steps after loop..&gt;&gt;</code></pre>
<p>Thus,</p>
<pre><code>    size(program 8b loop)     = size(init loop) + size(if)
    Now, let  size(init loop) = i, some nonzero size depending on the type and number of operations

              size(if)        = size(condition) + size(if branch) + size(else branch)
                              = size(condition) + size(goto end) + [size(loop body) + size(increment loop) + size(goto top)]
                              = c + 1* + [ b  + p + 1* ]

                     where  c = a nonzero size for the condition check
                            b = a variable loop body size
                            p is some variable nonzero size for the increment step

                              = c + b + p + 2*
                              = c + b + p + 2*
    Thus,
              size(loop)      = i + c + b + p + 2*                                   --(21)</code></pre>
<p>... which still comes out to a clean enough &quot;sum of parts&quot; type of equation. </p>
<p>Back to program 8, however; for we were trying to determine the size of <em>that</em> loop. Here&#39;s its template form:</p>
<pre><code>    // program 8-templatized
    loop &lt;&lt;loop init&gt;&gt; to &lt;&lt;loop condition&gt;&gt; &lt;&lt;implicit loop increment&gt;&gt;
      &lt;&lt;loop body&gt;&gt;
    end loop</code></pre>
<p>At this level of abstraction, all the other elements of the loop are present, but there are no gotos. The gotos are somehow &quot;subsumed&quot; in the mechanism of the loop such that the user of the loop doesn&#39;t have to know about it. So at this level, the size of a loop would be:</p>
<pre><code>    size(program 3 loop)   = i + c + b + p                                           --(22)</code></pre>
<p>What about the other forms of loops that we alluded to earlier? Here&#39;s the template form of the other typical versions of the loop: the <code>while</code> and the <code>do-while</code>:</p>
<pre><code>    // while-template                        // do-while-template
    &lt;&lt;loop init&gt;&gt;                            &lt;&lt;loop init&gt;&gt;
    while &lt;&lt;loop condition&gt;&gt;                 do
        &lt;&lt;loop body&gt;&gt;                           &lt;&lt;loop body&gt;&gt;
        &lt;&lt;loop increment&gt;&gt;                      &lt;&lt;loop increment&gt;&gt;
    end while                                while &lt;&lt;loop condition&gt;&gt;</code></pre>
<p>All are functionally equivalent and from inspection it&#39;s obvious that they have the same size (even if we count the <code>c</code> piece in different order). They are structurally different, but that&#39;s not germane to their size. </p>
<p>So to summarize the discourse on loops:</p>
<pre><code>    size(loops with goto) = i + c + b + p + 2*
    size(for loop)        = i + c + b + p
    size(while loop)      = i + c + b + p
    size(do while loop)   = i + c + b + p                                            --(23)</code></pre>
<p>Or more generally,</p>
<pre><code>    size(loop)            = i + c + b + p + o
                  where o = overhead at level of abstraction
                              = 0 at SSI level
                              = 2* if gotos are explicitly used                      --(24)</code></pre>
<p>To answer the specific question of Program 8&#39;s size, however, we&#39;ll have to apply (24) and make some assumptions on the sizes again. Since both the initialization and increment steps in this case are single operations, we&#39;ll assume they&#39;re also of size 1 sq. unit. The loop body consisting of the single print operation has long been deemed of size 1 sq. unit; and the condition is simple enough so we&#39;ll take that too to be a size 1. Now applying these values we get:</p>
<pre><code>    size(program 8)  = size(loop) + size(stop)
                     = i  + c  + b  + p  + o + 1*
                     = 1* + 1* + 1* + 1* + 0 + 1*
                     = 5* sq. units vs SLOC: 3</code></pre>
<h3>Summary</h3>
<p>Let&#39;s review: we started off by using the simple SSI model because it was simple and could easily be related to most basic operations. We focused on sizing programs, wrote 3 programs in pseudo-code and came up with formulas for the 3 operations in the model using a (yet unnamed) new measure of size for them. Here&#39;re the sizes that we arrived at using the new formulas and measure:</p>
<pre><code>    size(program 6) = 6* sq. units vs SLOC of 6
    size(program 7) = 6* sq. units vs SLOC of 7
    size(program 8) = 5* sq. units vs SLOC of 3                                       --(25)</code></pre>
<h4>Comparison to SLOC</h4>
<p>Admittedly, there&#39;s a lot of hand-waving going on here; but the contrast with SLOC is interesting. The 2D-ness of our size unit seems to hint at the structure that goes into loops and conditionals better than the &quot;flat&quot; count of lines. Program 7&#39;s size is smaller compared to its SLOC because the textual &quot;overhead&quot; of denoting loop structure is removed from the equation, while Program 8&#39;s textual brevity in having all structural elements of the
loop contained in the same line is called out, increasing its size when compared to its SLOC.</p>
<h4>Combining Sequence, Selection and Iteration</h4>
<p>SSI essentially talks about programs as being <em>combinations</em> of sequence, selection and iteration; while we&#39;ve been doing all our analysis in isolation. Let&#39;s fix that. Our formulas for sizes of these individual operations are:</p>
<pre><code>    size(sequence) = sum(size(operation)) for all operations in sequence       --(2&#39;) program replaced with sequence
    size(if)       = c + sum(b)                                                --(14) 
    size(loop)     = i + c + b + p + o                                         --(24)</code></pre>
<p>Since all these formulas are additions, we <em>could</em> treat the <code>if</code>s and <code>loop</code>s as &quot;compound&quot; operations that contribute a known size to a larger sequence that is the program; and at this larger level, there is only the sequence. This means that (2&#39;) above can be restored back to its original form:</p>
<pre><code>    size(program) = sum(size(operation)) for all operations in program
                  = sum(size(ifs)) + sum(size(loops)) + sum(size(other operations))     --(26)</code></pre>
<p><em>Now</em> we&#39;re getting somewhere. We started off with the expectation that the size of the whole would be the sum of the size of the parts; and we&#39;ve reached that result for the three primordial operations. If we are able to shore up those pesky asterisks we&#39;ve had to put on the &quot;other&quot; operations also as sums of sizes, we&#39;d be golden.</p>
<p>Also note that as long as all we&#39;re doing is measuring size, the order of operations don&#39;t seem to matter - only that they are counted. To use the marble run analogy, the size or color of the blocks don&#39;t matter - only their number. Obviously, this will not apply for other measures of code such as complexity: we build code in a particular order explicitly so as to effect certain results; but for size this is a Good Thing(TM).</p>
<h4>Levels of Abstraction</h4>
<p>The discussion above kept moving between alternate ways of representing things: Program 7 was represented as programs 7A and Program 8 as 8B to depict the same program with different constructs. These 2 ways can be thought of - especially within the Structured Programming context - as two different languages: the former being a pure structured program with no gotos and the latter as a &quot;lower level&quot;, unstructured language that DOES have gotos. This hints at two things that we&#39;ll explore soon:</p>
<ul>
<li>The size of a construct could differ by level of abstraction even if the two representations are the same in terms of function and semantics.</li>
<li>Constructs that are considered &quot;atomic&quot; at one level could be &quot;compound&quot; at another.</li>
</ul>
<h4>Moving beyond SSI</h4>
<p> The SSI model has been a good deep dive into simple operations, but at the level of abstraction that SSI is described, the <code>if</code> and the <code>loop</code> are the only concepts required to express general computing, so &quot;what each step in a sequence does&quot; is glossed over. To size an actual codebase, however, we do need to consider each such operation and size them. Up until now we&#39;ve been making assumptions on their size using those pesky asterisks, but they <em>do</em> need to go.</p>
<p><a name="turtles"></a></p>
<h2>Turtles all the way</h2>
<p>Ok, so it&#39;s finally time to address the size of operations. Consider a snippet like 2.1 (repeated below):</p>
<pre><code>    // snippet 2.1
    PrintStream outPS;                 // line 1.1.1
    outPS = System.out;                // line 1.1.2
    outPS.println(&quot;Hello World&quot;);      // line 1.2
    // SLOC : 3, Size : ?</code></pre>
<p>It has a variable declaration (1.1.1), an assignment(1.1.2) and a member access combined with a function call (1.2). Each of these operations don&#39;t <em>feel</em> like they could be the same size simply because I immediately begin to imagine their implementation and know that they must be inherently different in implementation and therefore of different sizes.</p>
<p>These questions are still within the same language. With real-world code we will have further issues:</p>
<ol>
<li>When we start comparing one language with another we will have to answer questions like &quot;How does a Hello World in Java compare to one in Ruby?&quot;, which will quickly devolve into &quot;How many Java print statements equals one Ruby print&quot; and such like.</li>
<li>When we consider applications written using multiple languages (as most applications are today) we might want a universal size metric that applies across all statements in all languages to measure size uniformly. More importantly, it will come handy when measuring two solutions to the same problem using different technology stacks.</li>
</ol>
<p>Notice, however, that we&#39;ve again started talking about the base platforms upon which the languages are written and not the languages themselves? What if the implementation is not accessible to us? Or if it is actually hardware and therefore not interpreted or sized the same way?</p>
<h3>Down One level</h3>
<p>For argument&#39;s sake, let&#39;s say we do have access to the platform and can measure the sizes in that language. Would we be able to say anything more about the size of the program under consideration? Let&#39;s work it out:</p>
<p>For any language L with n unique Simple operations (including containers) S1, S2, ... Sn (hereafter abbreviated to <L,S,n>), the size of any program P in L is:</p>
<pre><code>    size(P) = K1.size(S1) + K2.size(S2) + ... + Kn.size(Sn)    
        = sum(Ki.size(Si)) for i: 1 to n                                             --(27)</code></pre>
<p>since any or all of the unique simple operations (and/or containers) will have to be used in <code>Ki</code> quantities to make the program.</p>
<p>If a language <code>&lt;L,S,n&gt;</code> is implemented in terms of another language <code>&lt;L&#39;,S&#39;,m&gt;</code>, each of <code>L</code>&#39;s unique simple operations S1...Sn can be themselves expressed as programs in L&#39;. Since the simple operations in L&#39; are s1,s2,....sm, we get:</p>
<pre><code>        size(S1) = sum(Kj.size(sj)) for j: 1 to m
|||ly,  size(S2) = sum(Kj.size(sj)) for j: 1 to m                                    --(28)</code></pre>
<p>... and so forth. </p>
<p>But L&#39; <em>itself</em> is most probably implemented in terms of another language, say L&#39;&#39;. So even if we were able to say anything definitive about the sizes of S1 and S2, it would be in terms of L&#39;&#39; sizes. L&#39;&#39; could very well be built using L&#39;&#39;&#39;. Where will this end?</p>
<h3>Down Infinite Levels and back</h3>
<p>To illustrate the true nature of this rabbit hole, I present an excerpt from the <a href="http://tbd/">Grand Unified Theory of Software Engineering, page TBD</a>:</p>
<blockquote>
<p>... Equally frustrating, also these executors of specifications are intangible. To start with, a software process, such as a Java Virtual Machine, is not only a consumer of Java Bytecode specifications, it is also itself an executing program specification, e.g., the file with the name <code>jvm.exe</code>. But if we explain the software process in terms of a specification, then this explanation only brings us back to the intangibility of the specification, so we have gained nothing in concreteness. Perhaps, then, we can become concrete by considering the executor of the executor. Typically, a java Virtual Machine is executed by a hardware processor; surely this must be a firm base on which we can base our discipline. Unfortunately, the relief of finding something tangible is short-lived. Because just like an electric capacitor is constructed as a model of the ideal capacitor, a processor is a (physical) model of an ideal processor. It is not important that the processor in a typical computer is implemented in silicon, for it could have been implemented using vacuum tubes, a small and nimble-fingered person, or yet another software program. So also the processor is in this sense independent of the medium in which it is constructed. In the words of Edsgar Dijkstra:</p>
<blockquote>
<p>Originally I viewed it as the function of the abstract machine to provide a truthful picture of the physical reality. Later, however, I learned to consider the abstract machine as the &quot;true&quot; one, because that is the only one we can &quot;think&quot;; it is the physical machine&#39;s purpose to supply &quot;a working model&quot;, a (hopefully) sufficiently accurate physical simulation of the true, abstract machine.</p>
</blockquote>
</blockquote>
<p>In other words, if we started this journey down the levels of implementation to try and understand the size of a Java function call, the next immediate step is the JVM source. From there we could proceed onto its Java or C source, the source of the C runtime, the source of the Assembly runtime that follows, onto the CPU, its microcodes and eventually onto the very NAND gates that make up the machine that runs the code. But even at this level, we&#39;d be sizing up something that doesn&#39;t exist physically but is merely modeling (as closely as possible within the 0-5V range) a concept that exists in solely somebody&#39;s head. </p>
<p>Why then should we attempt to find absolute size of code? Could we make do with a size that&#39;s relative to a known base below which we will not probe, but is sufficient for us to understand the size of whatever is built above it? Let&#39;s work it out.</p>
<p>In the formulas we&#39;ve been writing till now, this would be:</p>
<pre><code>    If &lt;L,S,n&gt;, &lt;L&#39;,S&#39;,n&#39;&gt;, &lt;L&#39;&#39;,S&#39;&#39;,n&#39;&#39;&gt;, ... &lt;L*,S*,N*&gt; are a set of languages such that 
    ... L1  is implemented in L&#39;
    ... L2 is implemented in L&#39;&#39;, and so forth till
    ... L* is a base language that we choose to ignore the implementation of.

    if s is an operation in S, its size in terms of L&#39; is:
    size(s)   = sum(Ki.size(S&#39;i)) for i: 1 to n&#39;
              = A1.size(s&#39;1) + A2.size(s&#39;2) + ... + An&#39;.size(s&#39;n&#39;)
               where Ai are constants as described in (28) above.

    Similarly, size of any operation in S&#39; in terms of L&#39;&#39; is:
    size(s&#39;1) = sum(Bj.size(s&#39;&#39;j)) for j: 1 to n&#39;&#39;
              = B1.size(s&#39;&#39;1) + B2.size(s&#39;&#39;2) + ... + Bn&#39;&#39;.size(s&#39;&#39;n&#39;&#39;)
              where Bj are constants as described in (28) above.

    Thus expressing each S&#39; in terms of L&#39;&#39;,
    size(s)  = A1  . sum( Bj.size(s&#39;&#39;j) for j: 1 to n&#39;&#39; ) +
               A2  . sum( Bj.size(s&#39;&#39;j) for j: 1 to n&#39;&#39; ) +
                ... +
               An&#39; . sum( Bj.size(s&#39;&#39;j) for j: 1 to n&#39;&#39; )
             = A1.B1. size(s&#39;&#39;1) + A2.B2.size(s&#39;&#39;2) + ... + An&#39;.Bn&#39;&#39;.size(s&#39;&#39;n&#39;&#39;)

    If we carried this exercise all the way to L*, the operations in S can be expressed in terms of L* as:
    size(s)  = A1 B1 ... K*1.size(s*1) + A1 B1 ... K*2.size(s*2) + .... + A1 B1 ... K*n*.size(s*n)
               where K*i is the set of constants at L* required to build an operation in S

                                                                                     --(29)</code></pre>
<p>By definition, L* is a language that we chose not to break down anymore. At this asymptote, size has no further detailed meaning because all operations just happen. They do what they do without resorting to further delegation. As such, they can all be deemed to be of the same size; the only constraint being that the value chosen should not be zero, since that would nullify the sizes of all operations based on these operations in L*. It therefore makes sense to set the sizes of the operations in L* to a unit size. That is,</p>
<pre><code>    size(s*i) = 1 for i: 1 to n*                                                     --(30)</code></pre>
<p>Thus the size of operations in L in terms of L* can be written as:</p>
<pre><code>    size(s) = A1 B1 ... K*1. 1 + A2 B2 ... K*2. 1 + ... + A1 B1 ... K*n* . 1
            =      C1          +       C2         + ... +      Cn*                   --(31)</code></pre>
<p>... which is a &quot;sum or parts&quot; equation. So we have (for the most part) achieved what we set out to - define code size as a sum of sizes of the atomic operations that make it up.</p>
<p>More importantly, we have also established that:</p>
<ul>
<li>There is no absolute size; all size is relative to the &quot;level of abstraction&quot;</li>
<li>We can pick an arbitrary level to be our &quot;base&quot; and attribute unit sizes to all atomic operations at that level</li>
<li>Once we have a base, all sizes for code above it - its applications, so to speak - can be measured as sums of the sizes of the atomic operations.</li>
</ul>
<p>So we&#39;re gravitating towards a 3-tier model of relative size:</p>
<pre><code>    app
     |
    lang
     |
    base</code></pre>
<p>In BNF-ish definitions and formulas again:</p>
<pre><code>    app      := program+       ... in potentially multiple languages
    program  := operation+     ... in one language
    language := operation+
    base     := language

    size(app)       = sum(size(program))
    size(program)   = sum(size(operation))
    size(operation) = size(program in base)  if language
                    = 1 unit wrt base        if base
                                                                                     --(32)</code></pre>
<p>Note that last bit: the unit no longer can be unadorned, we have to qualify it with &quot;with ... as base&quot; so that we always are comparing apples to apples.</p>
<p>Rewriting (31) thus, we have:</p>
<pre><code>size(s) = A1 . L1 . B1 + A2 . L2 . B2 . 1 + .... + An* . Ln* . Bn*
   where
     Ai = Application constants
     Li = Language constants
     Bi = Base constants = 1                                                         --(33)</code></pre>
<p><a name="back_up"></a></p>
<h2>Back up the rabbit hole...</h2>
<h3>... to SSI</h3>
<p>So let&#39;s try backing up to where we left off with the SSI model. Depicting the programs that we wrote in that section in the 3 tier model we get:</p>
<pre><code>program 6      program 7     program 8
   |              |              |
   +--------------|--------------+
                  |
            SSI pseudo lang
                  |
                  ?</code></pre>
<p>What do we choose as base for the SSI model? Since we explicitly stated it to be pseudo code, we could invent a base language if we wanted to, but we can also safely treat it as <strong>its own base</strong>. Now we can complete equation (27) with the size of operations using the result in (33): </p>
<pre><code>    size(if)                  = c + sum(b)                                           --(14) 
    size(loop)                = i + c + b + p + o                                    --(24)
    size(any other operation) = 1                                                    --(33)</code></pre>
<p>Now we can work out the sizes of the programs in pseudo-language units:</p>
<p>TODO: FILL THIS TABLE OUT</p>
<table>
<thead>
<tr>
<th>Program</th>
<th>SLOC</th>
<th>Size</th>
<th>Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td>program 6</td>
<td>6</td>
<td>6 units</td>
<td>The same as SLOC for &quot;linear&quot; programs</td>
</tr>
<tr>
<td>program 7</td>
<td>7</td>
<td>2+1+(1+1)+1 = 6 sq. units</td>
<td>Less than SLOC, but &quot;wider&quot;</td>
</tr>
<tr>
<td>program 8</td>
<td>4</td>
<td>1+1+1+1+0+1 = 5 sq. units</td>
<td>&quot;Wider&quot; than SLOC and thus larger numerically</td>
</tr>
</tbody>
</table>
<p>Note that the asterisks are now gone.</p>
<h3>.... to <code>HelloWorld</code></h3>
<p>Now let&#39;s try backup all the way to where we started: <code>Hello World</code>. Since it&#39;s a higher level than the simple SSI pseudo language, we have at least two options for a base:</p>
<pre><code>  Hello world   Hello World
      |             |
    Java           Java
      |             |
     JVM           Java</code></pre>
<p>The JVM is the obvious choice; it makes sense hierarchically and is the rightful base. However, if all we&#39;re doing is comparing Java programs, it might be easier to compare at an atomic Java statement level (similar to how we broke things down in that section above) instead of going down to the bytecodes that make up those statements.</p>
<p>TODO: FILL THIS TABLE OUT</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Size in Java units</th>
<th>Size in bytecode units</th>
</tr>
</thead>
<tbody>
<tr>
<td>Class</td>
<td>1</td>
<td>TBD</td>
</tr>
<tr>
<td>Method</td>
<td>1</td>
<td>TBD</td>
</tr>
<tr>
<td>Package import</td>
<td>1</td>
<td>TBD</td>
</tr>
<tr>
<td>function call</td>
<td>1</td>
<td>TBD</td>
</tr>
</tbody>
</table>
<p>Now we can calculate the size of the program and compare with SLOC:</p>
<table>
<thead>
<tr>
<th>Program</th>
<th>SLOC</th>
<th>Size in Java units</th>
<th>Size in bytecode units</th>
</tr>
</thead>
<tbody>
<tr>
<td>program 6</td>
<td>5</td>
<td>1+1+3 = 5 units</td>
<td>TBD</td>
</tr>
</tbody>
</table>
<p><a name="turing_unit"></a></p>
<h2>A new measure of code size</h2>
<p>Now that we have a way to measure code size, its seems only right to give it a name so we can treat size with some specificity. Of course, this is a relative measure, so it always has to be qualified with &quot;... in base b&quot; or &quot;... with respect to base b&quot;; and it has to be prefixed with the dimension of the size - whether it is linear or square or higher; but it still needs a name.</p>
<p>As we went through the rabbit hole discussion, I didn&#39;t mention it, but the one true asymptotic machine in CS lore is the Turning Machine - that primordial representation of a computer. So what better name for code size than <strong>Turing</strong>?</p>
<p>I give you therefore:</p>
<blockquote>
<p><strong>Turing</strong>: the unit for size of code. It is dimensional and relative to a base. 
Symbol: T|b
        where b is a language that is used as the foundation</p>
</blockquote>
<p>Acceptable usage of the unit in written text to describe code with linear size 15 with Java as base would be: <code>15T|Java</code> or <code>15T in Java</code> or <code>15 Java turings</code>.</p>
<h2>Tying up some loose ends in the brave new world of Turing-sized code</h2>
<h3>Comparing code across languages</h3>
<p>Let&#39;s reconsider some questions raised earlier:</p>
<ol>
<li><p>How would we compare a Hello World written in Java vs one written in Ruby, for example? <strong>Ans</strong>: This is the real reason for the 3rd tier of the relative model: to compare programs written in different languages. So to compare a Hello World program written in Java vs one in Ruby, we have to reduce them both to a common base, say an x86 instruction set. </p>
<pre><code> Java Hello World           Ruby Hello World
        |                           |
       Java                        Ruby
        |                           |
        +------------+--------------+
                     |
                   x86 PC   </code></pre>
<p>Of course, This is hardly easy because the language runtimes are in between and isolating <em>just those parts of the runtime that this program uses</em> is near impossible. However, it is possible to imagine a common conceptual machine, e.g., <a href="TBD">the LLVM</a> that both run on and derive meaningful results that way. Now that we have a way of comparing that is independent of the source format, its conceiveable that we arrive on a &quot;Standard Turing Machine&quot; with a standard set of operations that will be used to measure all code. However, this is ripe for &quot;subject to interpretation&quot; issues to creep in.</p>
</li>
<li>If size is reduced to 1 at a base language, will languages become bereft of meaning? Is &quot;Javaness&quot; or &quot;Ruby ness is purely from the structure and not meaning? <strong>Ans</strong>: Size is a gross metric. By explicitly setting all operations to have the same size at a chosen level of abstraction, we have chosen to ignore meaning for sizing purposes. So the operations don&#39;t loose their meaning, we&#39;re just ignoring them for sizing purposes. Of course, complex logic is likely to be larger, but that is as much of their &quot;meaning&quot; that Size will capture.</li>
<li>Can we use this model in reverse, i.e., to compare sizes of bases when implementing the same app? <strong>Ans</strong>: Yes, in one sense. If we built the same app in two different languages and bases, we could compare the bases in terms of fit for that app.</li>
</ol>
<h3>Platform affordances</h3>
<p>Earlier in this chapter we saw two examples of platform affordances: in Java we dont need to declare a <code>PrintStream</code> if we&#39;re using the default one, i.e, <code>System.out</code> and in Ruby we dont need a class or main method if we&#39;re using the default <code>Class</code> module that the run time wraps around scripts automatically. These are also decidedly &quot;base&quot; functionality that affects the calculation of size in that counting them in or out could change the size dramatically. Couple of interesting threads lead from here:</p>
<ol>
<li>The ease of development (aka programmer productivity) is usually expressed in terms of the size of the app layer. This is because the language and base are a given at that point. A language is verbose or not depending on how much it subsumes within itself to reduce the size of the app layer. So if it affords a easier app development style by &quot;hiding&quot; sensible defaults in the language layer, this is a good thing for productivity, but doesn&#39;t essentially reduce the overall size. In terms of size, Platform affordances would be no different from boilerplate code generated by an IDE, for example. However, some of it could be considered &quot;dynamic size&quot;.</li>
<li>The &quot;iceberg&quot; nature of code: How much is visible and how much submerged? The size of the language+base represents the difficulty to move from one platform to another.</li>
</ol>
<h3>The dimension of size</h3>
<p>We made a natural progression from linear size to two-dimensional size through this chapter. The nature of software is that it could get to higher dimensions - we are limited only by our imagination when creating code to &quot;make the marble run as complex as we want&quot;. One good approximation of how complex software can get in real life is IC&#39;s - integrated circuits are literal representations of code paths and they routinely stack multiple 3-D layers to build single chips. In one sense, Alan Kay&#39;s representation of the Windows code as pages of text is not that far-fetched as a crude 3-D model of code that&#39;s size using Turings. </p>
<p>TODO: FILL THIS OUT AFTER ADDING THE GRAPH VIZ PIECES, SO IT MAKES SENSE.</p>
<h2>Engineer&#39;s Corner</h2>
<p>Ok, how do we operationalize this measure?</p>
<p>[STOPPED HERE SEP 25 PM]</p>
<p>For small programs like the ones above, counting operations is fine, but for any non-trivial codebase we cannot expect to get a true count of atomic statements without some medium-to-high complexity parsing of the source - something that might not be acceptable in all cases. If we instead took the shortcut of just considering line 1 a single statement, its size is 1. So: we can take one of two stances to answer Question #1:</p>
<ol>
<li><strong>The Simple Way</strong>: A statement is whatever appears between two statement separators per the language&#39;s grammar.</li>
<li><strong>The Exact Way</strong>: A statement is quite literally the simplest statement that could be written in the language; any time you can convert a statement in the language&#39;s grammar into a set of smaller statements within that same grammar that effectively does the same thing, it cannot be considered an atomic <em>Statement</em>. It&#39;s a Compound Statement; and the size of such a statement is the sum of the sizes of all the atomic statements that replace it.</li>
</ol>
<p>The Simple way is good in that it is easily applicable - both manually and with tools; and bad in that expressive languages can &quot;pack a lot of wallop&quot; into a single complicated statement (I&#39;m looking at you, APL) which will not be represented truthfully in the size of the program. But it can be considered the next incremental step to counting SLOC - with the statement separator replacing the newlines.</p>
<p>The Exact way is good in that it tends towards purity in measurement - code can be sized in terms of the atomic operations supported by the language itself and/or its runtime. It does, however, require non-trivial understanding/parsing of the source by human/tool to arrive at the exact size of code.</p>
<p>Should we pick one method over the other? The scientific mind suggests discarding the Simple for the Exact. The Simple approach, however, might be useful for &quot;rough estimates&quot;. It could also be that the difference between sizes arrived at by the two methods are statistically close enough for a sufficiently large body of code that we might not want to go through the pain of calculating size exactly.</p>
<p>So let&#39;s keep both for now.</p>
<p>Yes, this does mean that we&#39;re comparing apples to oranges; but let&#39;s see how far it takes us.</p>
<p>So, applying the formula</p>
<pre><code>    size(code in container) = size(container) + sum(size(contents)) for all contents in the container</code></pre>
<p>...to program 1, we get:</p>
<pre><code>    size(program1) = size(program) + size(main)
                   =      1        + size(main)
    Now,
     size(main)    =      1        + size(statements)
     size(stmts)   =      1 if using simple approach
                   =      3 if using exact  approach
    Thus,
    size(program1) = 1 + 1 + 1 = 3 (simple)
                   = 1 + 1 + 3 = 5 (exact)</code></pre>
<p>Doing the same exercise for all the programs we&#39;ve written so far gives us:</p>
<p>Table 1</p>
<table cellpadding="1" border="1">
<tr align="middle" valign="top">
    <th align="left"> Program </th>
    <th> SLOC </th>
    <th> Size(Simple) </th>
    <th> Size(Exact) </th>
    <th> Comments </th>
</tr>
<tr align="right" valign="top">
    <td align="left"> Program 1: HelloWorld.java </td>
    <td> 5 </td>
    <td> 3 </td>
    <td> 5 </td>
    <td align="left"></td>
</tr>
<tr align="right" valign="top">
    <td align="left"> Program 2: HW.java with SOP split </td>
    <td> 7 </td>
    <td> 5 </td>
    <td> 6 </td>
    <td align="left"> 
        size(import) = 1. <br/>
        Exact size(line 1.1) = 2 because it&#39;s eventually equivalent to the first 2 lines in snippet 2.1 
    </td>
</tr>
<tr align="right" valign="top">
    <td align="left"> Program 3: HW.java with 2 SOP calls </td>
    <td> 6 </td>
    <td> 4 </td>
    <td> 8 or 6 ? <br/> Ans: 8</td>
    <td align="left"> 
        Should the two System.outs be expanded with duplication or not? <br/>
        If yes, each line has size 3; if duplicates are removed, the two lines have a combined size of 4<br/>
        Ans: Yes, line has size 3. See below for discussion.
    </td>
</tr>
<tr align="right" valign="top">
    <td align="left"> Program 4: Hello World in Ruby </td>
    <td> 1 </td>
    <td> 1 </td>
    <td> 1 </td>
    <td align="left"> Is a Ruby/Python size 1 the same as a Java size 1? </td>
</tr>
<tr align="right" valign="top">
    <td align="left"> Program 5: HW.java with greet() </td>
    <td> 8 </td>
    <td> 5 </td>
    <td> 7 </td>
    <td align="left"> 
        size(greet)=1. <br/>
        size(SOP)=1 in simple and 3 in exact as before 
    </td>
</tr>
</table>

<p>Ok, that was for the smallest possible programs. What if we did the same exercise for slightly larger programs?</p>
<p>TODO: COMPARE SIZES OF 2 PROBLEMS IN THE SAME LANGUAGE
TODO: COMPARE SIZES OF SAME PROBLEM IN 4 LANGUAGES - EITHER 99 BOTTLES OR FIZZBUZZ</p>


</body>
</html>
